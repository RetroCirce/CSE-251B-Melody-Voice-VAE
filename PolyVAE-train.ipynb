{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this file \n",
    "# we train our proposed PolyVAE\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.distributions import kl_divergence, Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from model import PolyVAE\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class MinExponentialLR(ExponentialLR):\n",
    "    def __init__(self, optimizer, gamma, minimum, last_epoch=-1):\n",
    "        self.min = minimum\n",
    "        super(MinExponentialLR, self).__init__(optimizer, gamma, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "###############################\n",
    "# initial parameters\n",
    "s_dir = \"\"\n",
    "batch_size = 64\n",
    "n_epochs = 1\n",
    "data_path = [s_dir + \"data/poly_train.npy\",\n",
    "             s_dir + \"data/poly_validate.npy\",\n",
    "             s_dir + \"data/poly_train.npy\"]\n",
    "save_path = \"\"\n",
    "lr = 1e-4\n",
    "decay = 0.9999\n",
    "hidden_dims = 512\n",
    "z_dims = 1024\n",
    "vae_beta = 0.1\n",
    "input_dims = 130\n",
    "seq_len = 10 * 16\n",
    "beat_num = 10\n",
    "tick_num = 16\n",
    "save_period = 1\n",
    "##############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58041, 160])\n",
      "torch.Size([7578, 160])\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "train_set = np.load(data_path[0], allow_pickle = True)\n",
    "validate_set = np.load(data_path[1],allow_pickle = True) \n",
    "\n",
    "train_x = []\n",
    "for i,data in enumerate(train_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    train_x.append(temp)\n",
    "train_x = np.array(train_x)\n",
    "# print(train_x.shape)\n",
    "\n",
    "validate_x = []\n",
    "for i,data in enumerate(validate_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    validate_x.append(temp)\n",
    "validate_x = np.array(validate_x)\n",
    "# print(train_x.shape)\n",
    "train_x = torch.from_numpy(train_x).long()\n",
    "validate_x = torch.from_numpy(validate_x).long()\n",
    "\n",
    "print(train_x.size())\n",
    "print(validate_x.size())\n",
    "\n",
    "train_set = TensorDataset(train_x)\n",
    "validate_set = TensorDataset(validate_x)\n",
    "\n",
    "train_set = DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "validate_set = DataLoader(\n",
    "    dataset = validate_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "model = PolyVAE(input_dims, hidden_dims, z_dims, seq_len, beat_num, tick_num, 4000)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "if decay > 0:\n",
    "    scheduler = MinExponentialLR(optimizer, gamma = decay, minimum = 1e-5)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using: CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "# process validete data from the dataloder\n",
    "validate_data = []\n",
    "for i,d in enumerate(validate_set):\n",
    "    validate_data.append(d[0])\n",
    "print(len(validate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def std_normal(shape):\n",
    "    N = Normal(torch.zeros(shape), torch.ones(shape))\n",
    "    if torch.cuda.is_available():\n",
    "        N.loc = N.loc.cuda()\n",
    "        N.scale = N.scale.cuda()\n",
    "    return N\n",
    "\n",
    "def loss_function(recon, target, r_dis, beta):\n",
    "    CE = F.cross_entropy(recon.view(-1, recon.size(-1)), target, reduction = \"mean\")\n",
    "#     rhy_CE = F.nll_loss(recon_rhythm.view(-1, recon_rhythm.size(-1)), target_rhythm, reduction = \"mean\")\n",
    "    normal1 =  std_normal(r_dis.mean.size())\n",
    "    KLD1 = kl_divergence(r_dis, normal1).mean()\n",
    "    max_indices = recon.view(-1, recon.size(-1)).max(-1)[-1]\n",
    "#     print(max_indices)\n",
    "    correct = max_indices == target\n",
    "    acc = torch.sum(correct.float()) / target.size(0)\n",
    "    return acc, CE + beta * (KLD1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "__________________________________________\n",
      "batch 0 loss: 5.21777 acc: 0.00088 | val loss 5.19814 acc: 0.00000 iteration: 1\n",
      "batch 1 loss: 5.17792 acc: 0.05947 | val loss 5.16774 acc: 0.26045 iteration: 2\n",
      "batch 2 loss: 5.14806 acc: 0.34961 | val loss 5.10428 acc: 0.35947 iteration: 3\n",
      "batch 3 loss: 5.11382 acc: 0.35205 | val loss 5.01315 acc: 0.51250 iteration: 4\n",
      "batch 4 loss: 5.06965 acc: 0.39336 | val loss 5.00198 acc: 0.41602 iteration: 5\n",
      "batch 5 loss: 5.03020 acc: 0.38682 | val loss 4.91021 acc: 0.46875 iteration: 6\n",
      "batch 6 loss: 5.01098 acc: 0.35088 | val loss 4.76562 acc: 0.58623 iteration: 7\n",
      "batch 7 loss: 4.95353 acc: 0.37754 | val loss 4.85851 acc: 0.35967 iteration: 8\n",
      "batch 8 loss: 4.92823 acc: 0.34629 | val loss 4.89572 acc: 0.28291 iteration: 9\n",
      "batch 9 loss: 4.86625 acc: 0.36562 | val loss 4.76055 acc: 0.36572 iteration: 10\n",
      "batch 10 loss: 4.81285 acc: 0.35908 | val loss 4.52244 acc: 0.45596 iteration: 11\n",
      "batch 11 loss: 4.72808 acc: 0.36924 | val loss 4.39923 acc: 0.45020 iteration: 12\n",
      "batch 12 loss: 4.63723 acc: 0.36465 | val loss 4.71988 acc: 0.25352 iteration: 13\n",
      "batch 13 loss: 4.59770 acc: 0.33213 | val loss 4.63043 acc: 0.24980 iteration: 14\n",
      "batch 14 loss: 4.47322 acc: 0.34053 | val loss 3.94465 acc: 0.46309 iteration: 15\n",
      "batch 15 loss: 4.35406 acc: 0.34023 | val loss 3.75893 acc: 0.47168 iteration: 16\n",
      "batch 16 loss: 4.12205 acc: 0.37207 | val loss 3.55110 acc: 0.48467 iteration: 17\n",
      "batch 17 loss: 4.03301 acc: 0.36445 | val loss 3.88226 acc: 0.38330 iteration: 18\n",
      "batch 18 loss: 3.91787 acc: 0.37461 | val loss 4.32043 acc: 0.32246 iteration: 19\n",
      "batch 19 loss: 3.99101 acc: 0.36826 | val loss 4.07288 acc: 0.38389 iteration: 20\n",
      "batch 20 loss: 4.20263 acc: 0.34014 | val loss 4.06508 acc: 0.40107 iteration: 21\n",
      "batch 21 loss: 3.93727 acc: 0.41611 | val loss 4.39418 acc: 0.35762 iteration: 22\n",
      "batch 22 loss: 4.18152 acc: 0.36816 | val loss 3.58004 acc: 0.48955 iteration: 23\n",
      "batch 23 loss: 4.32683 acc: 0.34014 | val loss 4.03524 acc: 0.41064 iteration: 24\n",
      "batch 24 loss: 4.17477 acc: 0.35830 | val loss 3.28771 acc: 0.51250 iteration: 25\n",
      "batch 25 loss: 4.03436 acc: 0.36455 | val loss 3.13160 acc: 0.52959 iteration: 26\n",
      "batch 26 loss: 4.03584 acc: 0.34365 | val loss 3.34106 acc: 0.48418 iteration: 27\n",
      "batch 27 loss: 3.99705 acc: 0.34199 | val loss 3.35188 acc: 0.46035 iteration: 28\n",
      "batch 28 loss: 3.83660 acc: 0.35566 | val loss 3.47116 acc: 0.41865 iteration: 29\n",
      "batch 29 loss: 3.76429 acc: 0.36279 | val loss 4.24446 acc: 0.20996 iteration: 30\n",
      "batch 30 loss: 3.86776 acc: 0.34688 | val loss 3.47816 acc: 0.42988 iteration: 31\n",
      "batch 31 loss: 3.93727 acc: 0.32285 | val loss 3.43070 acc: 0.45820 iteration: 32\n",
      "batch 32 loss: 3.74982 acc: 0.36855 | val loss 4.07904 acc: 0.30898 iteration: 33\n",
      "batch 33 loss: 3.78919 acc: 0.36514 | val loss 3.82415 acc: 0.36904 iteration: 34\n",
      "batch 34 loss: 3.85050 acc: 0.33789 | val loss 4.21107 acc: 0.28965 iteration: 35\n",
      "batch 35 loss: 3.76924 acc: 0.35498 | val loss 3.41467 acc: 0.42744 iteration: 36\n",
      "batch 36 loss: 3.83372 acc: 0.33203 | val loss 3.00096 acc: 0.50078 iteration: 37\n",
      "batch 37 loss: 3.74862 acc: 0.35498 | val loss 4.11544 acc: 0.18594 iteration: 38\n",
      "batch 38 loss: 3.73367 acc: 0.34316 | val loss 3.99668 acc: 0.29668 iteration: 39\n",
      "batch 39 loss: 3.64947 acc: 0.36309 | val loss 4.13388 acc: 0.26973 iteration: 40\n",
      "batch 40 loss: 3.44676 acc: 0.40625 | val loss 3.36202 acc: 0.39678 iteration: 41\n",
      "batch 41 loss: 3.52948 acc: 0.38105 | val loss 3.47703 acc: 0.36475 iteration: 42\n",
      "batch 42 loss: 3.64083 acc: 0.35977 | val loss 3.71977 acc: 0.33887 iteration: 43\n",
      "batch 43 loss: 3.74372 acc: 0.33008 | val loss 3.47537 acc: 0.41543 iteration: 44\n",
      "batch 44 loss: 3.46030 acc: 0.38936 | val loss 3.92422 acc: 0.34434 iteration: 45\n",
      "batch 45 loss: 3.53820 acc: 0.36289 | val loss 3.92405 acc: 0.36377 iteration: 46\n",
      "batch 46 loss: 3.69479 acc: 0.34619 | val loss 3.66446 acc: 0.35088 iteration: 47\n",
      "batch 47 loss: 3.44010 acc: 0.41309 | val loss 3.52490 acc: 0.27100 iteration: 48\n",
      "batch 48 loss: 3.57552 acc: 0.37852 | val loss 2.54848 acc: 0.63105 iteration: 49\n",
      "batch 49 loss: 3.39042 acc: 0.40557 | val loss 2.88427 acc: 0.55371 iteration: 50\n",
      "batch 50 loss: 3.55932 acc: 0.36377 | val loss 2.81486 acc: 0.56484 iteration: 51\n",
      "batch 51 loss: 3.58259 acc: 0.35352 | val loss 4.40662 acc: 0.19102 iteration: 52\n",
      "batch 52 loss: 3.58991 acc: 0.35029 | val loss 3.30353 acc: 0.43867 iteration: 53\n",
      "batch 53 loss: 3.68793 acc: 0.32275 | val loss 3.40366 acc: 0.42559 iteration: 54\n",
      "batch 54 loss: 3.61704 acc: 0.34912 | val loss 3.66880 acc: 0.33691 iteration: 55\n",
      "batch 55 loss: 3.69570 acc: 0.32754 | val loss 3.22190 acc: 0.43662 iteration: 56\n",
      "batch 56 loss: 3.66622 acc: 0.33174 | val loss 3.28574 acc: 0.41621 iteration: 57\n",
      "batch 57 loss: 3.48887 acc: 0.38027 | val loss 4.11182 acc: 0.33242 iteration: 58\n",
      "batch 58 loss: 3.66175 acc: 0.33271 | val loss 3.42013 acc: 0.44971 iteration: 59\n",
      "batch 59 loss: 3.57323 acc: 0.35381 | val loss 3.09556 acc: 0.49277 iteration: 60\n",
      "batch 60 loss: 3.48650 acc: 0.36738 | val loss 2.68456 acc: 0.58662 iteration: 61\n",
      "batch 61 loss: 3.53303 acc: 0.35732 | val loss 2.80493 acc: 0.56055 iteration: 62\n",
      "batch 62 loss: 3.52126 acc: 0.35537 | val loss 3.26148 acc: 0.35439 iteration: 63\n",
      "batch 63 loss: 3.33084 acc: 0.39141 | val loss 3.60227 acc: 0.32139 iteration: 64\n",
      "batch 64 loss: 3.47197 acc: 0.36875 | val loss 3.63432 acc: 0.32588 iteration: 65\n",
      "batch 65 loss: 3.49480 acc: 0.35439 | val loss 3.86134 acc: 0.32803 iteration: 66\n",
      "batch 66 loss: 3.65564 acc: 0.31963 | val loss 4.11920 acc: 0.24160 iteration: 67\n",
      "batch 67 loss: 3.40810 acc: 0.37988 | val loss 3.13380 acc: 0.39131 iteration: 68\n",
      "batch 68 loss: 3.64385 acc: 0.31123 | val loss 3.18584 acc: 0.41807 iteration: 69\n",
      "batch 69 loss: 3.70062 acc: 0.30342 | val loss 3.23871 acc: 0.42832 iteration: 70\n",
      "batch 70 loss: 3.57725 acc: 0.33701 | val loss 3.12179 acc: 0.45635 iteration: 71\n",
      "batch 71 loss: 3.42916 acc: 0.38047 | val loss 2.96509 acc: 0.47656 iteration: 72\n",
      "batch 72 loss: 3.52951 acc: 0.34033 | val loss 3.04393 acc: 0.42109 iteration: 73\n",
      "batch 73 loss: 3.34377 acc: 0.39102 | val loss 3.10212 acc: 0.44648 iteration: 74\n",
      "batch 74 loss: 3.51865 acc: 0.34424 | val loss 3.12664 acc: 0.44531 iteration: 75\n",
      "batch 75 loss: 3.41788 acc: 0.37061 | val loss 3.64787 acc: 0.34941 iteration: 76\n",
      "batch 76 loss: 3.35822 acc: 0.38037 | val loss 3.58385 acc: 0.29385 iteration: 77\n",
      "batch 77 loss: 3.50581 acc: 0.34131 | val loss 2.69740 acc: 0.54727 iteration: 78\n",
      "batch 78 loss: 3.58271 acc: 0.32559 | val loss 3.60567 acc: 0.32773 iteration: 79\n",
      "batch 79 loss: 3.47239 acc: 0.35068 | val loss 4.06540 acc: 0.16777 iteration: 80\n",
      "batch 80 loss: 3.44186 acc: 0.34883 | val loss 3.77750 acc: 0.20518 iteration: 81\n",
      "batch 81 loss: 3.33228 acc: 0.38320 | val loss 3.82447 acc: 0.12451 iteration: 82\n",
      "batch 82 loss: 3.47789 acc: 0.33936 | val loss 3.80297 acc: 0.30576 iteration: 83\n",
      "batch 83 loss: 3.50075 acc: 0.32881 | val loss 3.72033 acc: 0.30908 iteration: 84\n",
      "batch 84 loss: 3.42887 acc: 0.34658 | val loss 3.86502 acc: 0.20977 iteration: 85\n",
      "batch 85 loss: 3.31932 acc: 0.38145 | val loss 3.42241 acc: 0.37900 iteration: 86\n",
      "batch 86 loss: 3.54173 acc: 0.31279 | val loss 3.37340 acc: 0.36553 iteration: 87\n",
      "batch 87 loss: 3.48870 acc: 0.35059 | val loss 3.62151 acc: 0.34385 iteration: 88\n",
      "batch 88 loss: 3.38022 acc: 0.36963 | val loss 3.55480 acc: 0.35947 iteration: 89\n",
      "batch 89 loss: 3.46462 acc: 0.34824 | val loss 3.74566 acc: 0.30098 iteration: 90\n",
      "batch 90 loss: 3.45518 acc: 0.35322 | val loss 3.60512 acc: 0.29395 iteration: 91\n",
      "batch 91 loss: 3.36524 acc: 0.37490 | val loss 3.38674 acc: 0.33828 iteration: 92\n",
      "batch 92 loss: 3.55829 acc: 0.33379 | val loss 3.74183 acc: 0.26670 iteration: 93\n",
      "batch 93 loss: 3.43255 acc: 0.33545 | val loss 3.89164 acc: 0.28936 iteration: 94\n",
      "batch 94 loss: 3.47818 acc: 0.35254 | val loss 4.03081 acc: 0.26904 iteration: 95\n",
      "batch 95 loss: 3.18230 acc: 0.41689 | val loss 3.95086 acc: 0.24941 iteration: 96\n",
      "batch 96 loss: 3.22879 acc: 0.39287 | val loss 2.79975 acc: 0.47676 iteration: 97\n",
      "batch 97 loss: 3.22905 acc: 0.40645 | val loss 2.86674 acc: 0.46006 iteration: 98\n",
      "batch 98 loss: 3.36197 acc: 0.37061 | val loss 3.68063 acc: 0.26475 iteration: 99\n",
      "batch 99 loss: 3.45440 acc: 0.34004 | val loss 3.33681 acc: 0.34102 iteration: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 100 loss: 3.32245 acc: 0.34756 | val loss 3.25845 acc: 0.40410 iteration: 101\n",
      "batch 101 loss: 3.43810 acc: 0.33896 | val loss 3.36108 acc: 0.39502 iteration: 102\n",
      "batch 102 loss: 3.28727 acc: 0.38887 | val loss 3.15915 acc: 0.43799 iteration: 103\n",
      "batch 103 loss: 3.46859 acc: 0.34424 | val loss 3.13755 acc: 0.45967 iteration: 104\n",
      "batch 104 loss: 3.31745 acc: 0.39189 | val loss 3.40719 acc: 0.31455 iteration: 105\n",
      "batch 105 loss: 3.37716 acc: 0.35742 | val loss 3.16309 acc: 0.44209 iteration: 106\n",
      "batch 106 loss: 3.28438 acc: 0.37412 | val loss 3.10720 acc: 0.45664 iteration: 107\n",
      "batch 107 loss: 3.45458 acc: 0.35107 | val loss 3.40893 acc: 0.34492 iteration: 108\n",
      "batch 108 loss: 3.19450 acc: 0.40430 | val loss 3.88756 acc: 0.21475 iteration: 109\n",
      "batch 109 loss: 3.37214 acc: 0.36172 | val loss 3.69774 acc: 0.28311 iteration: 110\n",
      "batch 110 loss: 3.06224 acc: 0.43877 | val loss 2.59375 acc: 0.50117 iteration: 111\n",
      "batch 111 loss: 3.34376 acc: 0.36025 | val loss 2.87626 acc: 0.43701 iteration: 112\n",
      "batch 112 loss: 3.37437 acc: 0.36543 | val loss 3.35961 acc: 0.28848 iteration: 113\n",
      "batch 113 loss: 3.20180 acc: 0.41914 | val loss 3.59010 acc: 0.23086 iteration: 114\n",
      "batch 114 loss: 3.20647 acc: 0.40088 | val loss 3.55520 acc: 0.36592 iteration: 115\n",
      "batch 115 loss: 3.46106 acc: 0.35830 | val loss 3.82689 acc: 0.32354 iteration: 116\n",
      "batch 116 loss: 3.36498 acc: 0.35146 | val loss 3.22798 acc: 0.32119 iteration: 117\n",
      "batch 117 loss: 3.21785 acc: 0.39502 | val loss 3.33138 acc: 0.34258 iteration: 118\n",
      "batch 118 loss: 3.23520 acc: 0.39443 | val loss 3.21361 acc: 0.38389 iteration: 119\n",
      "batch 119 loss: 3.26487 acc: 0.36777 | val loss 3.85159 acc: 0.27158 iteration: 120\n",
      "batch 120 loss: 3.23749 acc: 0.38330 | val loss 3.39349 acc: 0.37129 iteration: 121\n",
      "batch 121 loss: 3.13609 acc: 0.39336 | val loss 2.68231 acc: 0.52949 iteration: 122\n",
      "batch 122 loss: 3.30294 acc: 0.36270 | val loss 3.11976 acc: 0.42988 iteration: 123\n",
      "batch 123 loss: 3.23479 acc: 0.36074 | val loss 2.88266 acc: 0.46963 iteration: 124\n",
      "batch 124 loss: 3.19149 acc: 0.37559 | val loss 2.46945 acc: 0.58623 iteration: 125\n",
      "batch 125 loss: 3.35410 acc: 0.37305 | val loss 3.05755 acc: 0.36709 iteration: 126\n",
      "batch 126 loss: 3.37054 acc: 0.34141 | val loss 3.63865 acc: 0.28701 iteration: 127\n",
      "batch 127 loss: 3.06281 acc: 0.41855 | val loss 3.31641 acc: 0.37979 iteration: 128\n",
      "batch 128 loss: 3.10373 acc: 0.41963 | val loss 3.14154 acc: 0.46592 iteration: 129\n",
      "batch 129 loss: 3.21323 acc: 0.38877 | val loss 3.24383 acc: 0.49277 iteration: 130\n",
      "batch 130 loss: 3.29404 acc: 0.35000 | val loss 3.87812 acc: 0.28115 iteration: 131\n",
      "batch 131 loss: 3.28205 acc: 0.36777 | val loss 3.56607 acc: 0.27500 iteration: 132\n",
      "batch 132 loss: 3.18057 acc: 0.35664 | val loss 2.62217 acc: 0.47021 iteration: 133\n",
      "batch 133 loss: 3.19618 acc: 0.36240 | val loss 2.54898 acc: 0.49521 iteration: 134\n",
      "batch 134 loss: 3.29680 acc: 0.33174 | val loss 2.58948 acc: 0.48916 iteration: 135\n",
      "batch 135 loss: 3.09290 acc: 0.38564 | val loss 3.21549 acc: 0.38975 iteration: 136\n",
      "batch 136 loss: 3.16850 acc: 0.37520 | val loss 3.58943 acc: 0.33574 iteration: 137\n",
      "batch 137 loss: 3.29840 acc: 0.33779 | val loss 3.16259 acc: 0.40293 iteration: 138\n",
      "batch 138 loss: 3.11052 acc: 0.38799 | val loss 2.88678 acc: 0.42598 iteration: 139\n",
      "batch 139 loss: 3.17097 acc: 0.38027 | val loss 3.26404 acc: 0.36943 iteration: 140\n",
      "batch 140 loss: 3.16780 acc: 0.39434 | val loss 2.59188 acc: 0.51357 iteration: 141\n",
      "batch 141 loss: 3.00491 acc: 0.41846 | val loss 3.13648 acc: 0.42998 iteration: 142\n",
      "batch 142 loss: 3.06168 acc: 0.39658 | val loss 2.73052 acc: 0.51641 iteration: 143\n",
      "batch 143 loss: 3.13822 acc: 0.34873 | val loss 2.59731 acc: 0.54746 iteration: 144\n",
      "batch 144 loss: 3.16890 acc: 0.37832 | val loss 2.63957 acc: 0.49561 iteration: 145\n",
      "batch 145 loss: 2.96903 acc: 0.40830 | val loss 2.58327 acc: 0.47285 iteration: 146\n",
      "batch 146 loss: 3.16134 acc: 0.35732 | val loss 2.56021 acc: 0.44824 iteration: 147\n",
      "batch 147 loss: 3.09006 acc: 0.39189 | val loss 3.51486 acc: 0.23203 iteration: 148\n",
      "batch 148 loss: 3.23891 acc: 0.36553 | val loss 2.71861 acc: 0.45430 iteration: 149\n",
      "batch 149 loss: 2.99455 acc: 0.37793 | val loss 2.73473 acc: 0.45156 iteration: 150\n",
      "batch 150 loss: 3.11479 acc: 0.36670 | val loss 3.55053 acc: 0.33564 iteration: 151\n",
      "batch 151 loss: 3.10592 acc: 0.39658 | val loss 3.18123 acc: 0.40039 iteration: 152\n",
      "batch 152 loss: 3.28386 acc: 0.33867 | val loss 3.67961 acc: 0.29492 iteration: 153\n",
      "batch 153 loss: 3.13773 acc: 0.36328 | val loss 2.71256 acc: 0.43438 iteration: 154\n",
      "batch 154 loss: 3.11537 acc: 0.35791 | val loss 2.28746 acc: 0.50244 iteration: 155\n",
      "batch 155 loss: 3.18048 acc: 0.36533 | val loss 3.53420 acc: 0.19092 iteration: 156\n",
      "batch 156 loss: 2.87991 acc: 0.43896 | val loss 3.49944 acc: 0.33203 iteration: 157\n",
      "batch 157 loss: 3.10012 acc: 0.40908 | val loss 3.77286 acc: 0.27051 iteration: 158\n",
      "batch 158 loss: 2.99026 acc: 0.37676 | val loss 2.82482 acc: 0.42627 iteration: 159\n",
      "batch 159 loss: 3.07341 acc: 0.39199 | val loss 2.96743 acc: 0.38037 iteration: 160\n",
      "batch 160 loss: 3.09882 acc: 0.37314 | val loss 3.10601 acc: 0.36016 iteration: 161\n",
      "batch 161 loss: 3.09150 acc: 0.37939 | val loss 2.91172 acc: 0.41299 iteration: 162\n",
      "batch 162 loss: 3.16290 acc: 0.35537 | val loss 3.28381 acc: 0.38398 iteration: 163\n",
      "batch 163 loss: 3.18142 acc: 0.35879 | val loss 3.37880 acc: 0.38135 iteration: 164\n",
      "batch 164 loss: 2.96056 acc: 0.40244 | val loss 2.99688 acc: 0.40820 iteration: 165\n",
      "batch 165 loss: 3.02550 acc: 0.38320 | val loss 3.14681 acc: 0.30811 iteration: 166\n",
      "batch 166 loss: 3.08774 acc: 0.36807 | val loss 2.25207 acc: 0.62100 iteration: 167\n",
      "batch 167 loss: 3.06171 acc: 0.38701 | val loss 2.62203 acc: 0.54990 iteration: 168\n",
      "batch 168 loss: 3.13527 acc: 0.38887 | val loss 2.34097 acc: 0.56318 iteration: 169\n",
      "batch 169 loss: 3.07205 acc: 0.39395 | val loss 3.80667 acc: 0.28105 iteration: 170\n",
      "batch 170 loss: 2.99448 acc: 0.40127 | val loss 2.78238 acc: 0.52061 iteration: 171\n",
      "batch 171 loss: 3.09939 acc: 0.37227 | val loss 2.80426 acc: 0.51348 iteration: 172\n",
      "batch 172 loss: 3.08199 acc: 0.39824 | val loss 3.26683 acc: 0.36406 iteration: 173\n",
      "batch 173 loss: 3.01792 acc: 0.39561 | val loss 2.75241 acc: 0.45166 iteration: 174\n",
      "batch 174 loss: 3.09220 acc: 0.38262 | val loss 2.69774 acc: 0.46455 iteration: 175\n",
      "batch 175 loss: 2.98538 acc: 0.40156 | val loss 3.41107 acc: 0.40752 iteration: 176\n",
      "batch 176 loss: 3.03474 acc: 0.36729 | val loss 2.89983 acc: 0.47471 iteration: 177\n",
      "batch 177 loss: 3.16771 acc: 0.34873 | val loss 2.66431 acc: 0.49805 iteration: 178\n",
      "batch 178 loss: 3.10759 acc: 0.37373 | val loss 2.44301 acc: 0.57314 iteration: 179\n",
      "batch 179 loss: 3.17411 acc: 0.37832 | val loss 2.58601 acc: 0.53662 iteration: 180\n",
      "batch 180 loss: 2.91479 acc: 0.41104 | val loss 2.92001 acc: 0.37529 iteration: 181\n",
      "batch 181 loss: 3.10625 acc: 0.37324 | val loss 3.22008 acc: 0.36299 iteration: 182\n",
      "batch 182 loss: 3.02094 acc: 0.39883 | val loss 3.22946 acc: 0.39238 iteration: 183\n",
      "batch 183 loss: 3.01586 acc: 0.39346 | val loss 3.62884 acc: 0.29258 iteration: 184\n",
      "batch 184 loss: 3.17212 acc: 0.36250 | val loss 3.09769 acc: 0.42461 iteration: 185\n",
      "batch 185 loss: 3.11853 acc: 0.36387 | val loss 2.72143 acc: 0.40420 iteration: 186\n",
      "batch 186 loss: 3.14060 acc: 0.36553 | val loss 2.77446 acc: 0.44219 iteration: 187\n",
      "batch 187 loss: 3.05622 acc: 0.39512 | val loss 3.16629 acc: 0.42363 iteration: 188\n",
      "batch 188 loss: 2.92399 acc: 0.41895 | val loss 2.67946 acc: 0.48594 iteration: 189\n",
      "batch 189 loss: 3.13657 acc: 0.37988 | val loss 2.56816 acc: 0.50137 iteration: 190\n",
      "batch 190 loss: 3.16824 acc: 0.35410 | val loss 2.45614 acc: 0.44834 iteration: 191\n",
      "batch 191 loss: 2.90076 acc: 0.42793 | val loss 2.85060 acc: 0.43340 iteration: 192\n",
      "batch 192 loss: 3.04841 acc: 0.39482 | val loss 2.92085 acc: 0.41650 iteration: 193\n",
      "batch 193 loss: 3.11422 acc: 0.36094 | val loss 3.17239 acc: 0.35410 iteration: 194\n",
      "batch 194 loss: 3.06743 acc: 0.37236 | val loss 3.32330 acc: 0.29990 iteration: 195\n",
      "batch 195 loss: 3.10720 acc: 0.36201 | val loss 2.26805 acc: 0.55117 iteration: 196\n",
      "batch 196 loss: 3.12892 acc: 0.37480 | val loss 3.15719 acc: 0.39238 iteration: 197\n",
      "batch 197 loss: 3.18381 acc: 0.36807 | val loss 3.79617 acc: 0.22012 iteration: 198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 198 loss: 3.11952 acc: 0.37412 | val loss 3.59384 acc: 0.22383 iteration: 199\n",
      "batch 199 loss: 2.85728 acc: 0.39131 | val loss 3.84734 acc: 0.12783 iteration: 200\n",
      "batch 200 loss: 3.15916 acc: 0.35693 | val loss 3.35338 acc: 0.35869 iteration: 201\n",
      "batch 201 loss: 3.01121 acc: 0.39590 | val loss 3.44624 acc: 0.32568 iteration: 202\n",
      "batch 202 loss: 2.88698 acc: 0.41279 | val loss 3.60732 acc: 0.24697 iteration: 203\n",
      "batch 203 loss: 2.93188 acc: 0.39619 | val loss 3.07759 acc: 0.40293 iteration: 204\n",
      "batch 204 loss: 2.97237 acc: 0.40117 | val loss 3.23468 acc: 0.36689 iteration: 205\n",
      "batch 205 loss: 3.02435 acc: 0.37852 | val loss 3.12520 acc: 0.39678 iteration: 206\n",
      "batch 206 loss: 3.04444 acc: 0.39502 | val loss 3.01497 acc: 0.41094 iteration: 207\n",
      "batch 207 loss: 2.94775 acc: 0.40615 | val loss 3.12972 acc: 0.38877 iteration: 208\n",
      "batch 208 loss: 3.00978 acc: 0.39639 | val loss 3.24216 acc: 0.30938 iteration: 209\n",
      "batch 209 loss: 2.99803 acc: 0.37627 | val loss 3.10201 acc: 0.36357 iteration: 210\n",
      "batch 210 loss: 3.08324 acc: 0.37461 | val loss 3.47023 acc: 0.29326 iteration: 211\n",
      "batch 211 loss: 3.09266 acc: 0.36504 | val loss 3.62181 acc: 0.31221 iteration: 212\n",
      "batch 212 loss: 2.98378 acc: 0.38887 | val loss 3.67600 acc: 0.31260 iteration: 213\n",
      "batch 213 loss: 3.10964 acc: 0.35801 | val loss 3.58839 acc: 0.31514 iteration: 214\n",
      "batch 214 loss: 3.04678 acc: 0.36328 | val loss 2.54484 acc: 0.47920 iteration: 215\n",
      "batch 215 loss: 3.10073 acc: 0.36367 | val loss 2.57826 acc: 0.47314 iteration: 216\n",
      "batch 216 loss: 3.05244 acc: 0.36826 | val loss 3.53394 acc: 0.26504 iteration: 217\n",
      "batch 217 loss: 3.11375 acc: 0.38418 | val loss 3.16669 acc: 0.32559 iteration: 218\n",
      "batch 218 loss: 2.99210 acc: 0.40225 | val loss 2.88333 acc: 0.41367 iteration: 219\n",
      "batch 219 loss: 3.00438 acc: 0.38066 | val loss 2.99749 acc: 0.39912 iteration: 220\n",
      "batch 220 loss: 2.99719 acc: 0.38828 | val loss 2.70771 acc: 0.47490 iteration: 221\n",
      "batch 221 loss: 3.08820 acc: 0.35420 | val loss 2.73767 acc: 0.48564 iteration: 222\n",
      "batch 222 loss: 3.00168 acc: 0.39463 | val loss 3.14766 acc: 0.34824 iteration: 223\n",
      "batch 223 loss: 2.84347 acc: 0.42871 | val loss 2.76518 acc: 0.44424 iteration: 224\n",
      "batch 224 loss: 3.09452 acc: 0.37080 | val loss 2.73324 acc: 0.45977 iteration: 225\n",
      "batch 225 loss: 3.09512 acc: 0.36240 | val loss 3.14147 acc: 0.36162 iteration: 226\n",
      "batch 226 loss: 2.92561 acc: 0.40381 | val loss 3.64633 acc: 0.24482 iteration: 227\n",
      "batch 227 loss: 3.18344 acc: 0.36523 | val loss 3.36835 acc: 0.30186 iteration: 228\n",
      "batch 228 loss: 2.99117 acc: 0.41221 | val loss 2.32357 acc: 0.50313 iteration: 229\n",
      "batch 229 loss: 2.98391 acc: 0.38740 | val loss 2.66578 acc: 0.44854 iteration: 230\n",
      "batch 230 loss: 3.02746 acc: 0.38750 | val loss 3.20910 acc: 0.29824 iteration: 231\n",
      "batch 231 loss: 2.98176 acc: 0.40117 | val loss 3.43884 acc: 0.23809 iteration: 232\n",
      "batch 232 loss: 3.02720 acc: 0.38096 | val loss 3.35097 acc: 0.37080 iteration: 233\n",
      "batch 233 loss: 2.98507 acc: 0.38867 | val loss 3.62301 acc: 0.33604 iteration: 234\n",
      "batch 234 loss: 2.78821 acc: 0.40654 | val loss 2.94405 acc: 0.32949 iteration: 235\n",
      "batch 235 loss: 3.07739 acc: 0.38770 | val loss 3.13202 acc: 0.34531 iteration: 236\n",
      "batch 236 loss: 2.98254 acc: 0.37881 | val loss 3.10791 acc: 0.37354 iteration: 237\n",
      "batch 237 loss: 2.96338 acc: 0.39844 | val loss 3.90217 acc: 0.24531 iteration: 238\n",
      "batch 238 loss: 2.87621 acc: 0.42373 | val loss 3.24945 acc: 0.35107 iteration: 239\n",
      "batch 239 loss: 2.98992 acc: 0.40762 | val loss 2.25549 acc: 0.51621 iteration: 240\n",
      "batch 240 loss: 3.02977 acc: 0.39277 | val loss 2.97430 acc: 0.43652 iteration: 241\n",
      "batch 241 loss: 3.04999 acc: 0.35576 | val loss 2.70922 acc: 0.47295 iteration: 242\n",
      "batch 242 loss: 2.84842 acc: 0.40313 | val loss 2.18770 acc: 0.59180 iteration: 243\n",
      "batch 243 loss: 2.89719 acc: 0.37627 | val loss 2.90354 acc: 0.37510 iteration: 244\n",
      "batch 244 loss: 3.01265 acc: 0.37910 | val loss 3.47894 acc: 0.29209 iteration: 245\n",
      "batch 245 loss: 2.81549 acc: 0.40957 | val loss 3.12116 acc: 0.37715 iteration: 246\n",
      "batch 246 loss: 2.91757 acc: 0.39590 | val loss 2.68145 acc: 0.49795 iteration: 247\n",
      "batch 247 loss: 2.96831 acc: 0.38643 | val loss 2.48609 acc: 0.53369 iteration: 248\n",
      "batch 248 loss: 2.89906 acc: 0.40322 | val loss 3.68773 acc: 0.29453 iteration: 249\n",
      "batch 249 loss: 2.90539 acc: 0.38350 | val loss 3.39870 acc: 0.29688 iteration: 250\n",
      "batch 250 loss: 2.95441 acc: 0.37598 | val loss 2.36075 acc: 0.47988 iteration: 251\n",
      "batch 251 loss: 2.91085 acc: 0.42217 | val loss 2.25690 acc: 0.50166 iteration: 252\n",
      "batch 252 loss: 2.91022 acc: 0.39893 | val loss 2.34802 acc: 0.49873 iteration: 253\n",
      "batch 253 loss: 2.80622 acc: 0.41787 | val loss 2.78612 acc: 0.43359 iteration: 254\n",
      "batch 254 loss: 3.12706 acc: 0.32441 | val loss 3.14525 acc: 0.38916 iteration: 255\n",
      "batch 255 loss: 2.72204 acc: 0.42861 | val loss 2.54947 acc: 0.47354 iteration: 256\n",
      "batch 256 loss: 2.83073 acc: 0.41240 | val loss 2.57091 acc: 0.45039 iteration: 257\n",
      "batch 257 loss: 2.99292 acc: 0.37334 | val loss 3.15317 acc: 0.36934 iteration: 258\n",
      "batch 258 loss: 2.89383 acc: 0.37832 | val loss 2.27283 acc: 0.52266 iteration: 259\n",
      "batch 259 loss: 2.77931 acc: 0.41436 | val loss 2.94617 acc: 0.42715 iteration: 260\n",
      "batch 260 loss: 2.87299 acc: 0.38818 | val loss 2.37666 acc: 0.51543 iteration: 261\n",
      "batch 261 loss: 2.74300 acc: 0.43301 | val loss 2.20346 acc: 0.56035 iteration: 262\n",
      "batch 262 loss: 2.75064 acc: 0.41172 | val loss 2.38863 acc: 0.51250 iteration: 263\n",
      "batch 263 loss: 3.04590 acc: 0.35410 | val loss 2.43053 acc: 0.50313 iteration: 264\n",
      "batch 264 loss: 2.72993 acc: 0.43232 | val loss 2.46611 acc: 0.49082 iteration: 265\n",
      "batch 265 loss: 2.62658 acc: 0.44141 | val loss 3.43734 acc: 0.24854 iteration: 266\n",
      "batch 266 loss: 2.73492 acc: 0.40029 | val loss 2.58745 acc: 0.47549 iteration: 267\n",
      "batch 267 loss: 2.80787 acc: 0.40547 | val loss 2.63801 acc: 0.46973 iteration: 268\n",
      "batch 268 loss: 2.60323 acc: 0.44883 | val loss 3.05546 acc: 0.38018 iteration: 269\n",
      "batch 269 loss: 2.72639 acc: 0.40527 | val loss 2.65558 acc: 0.43613 iteration: 270\n",
      "batch 270 loss: 2.53821 acc: 0.44570 | val loss 3.26552 acc: 0.30107 iteration: 271\n",
      "batch 271 loss: 2.65978 acc: 0.39395 | val loss 2.56747 acc: 0.43721 iteration: 272\n",
      "batch 272 loss: 2.49541 acc: 0.42812 | val loss 2.25740 acc: 0.50898 iteration: 273\n",
      "batch 273 loss: 2.55428 acc: 0.41992 | val loss 3.41988 acc: 0.19971 iteration: 274\n",
      "batch 274 loss: 2.53235 acc: 0.41582 | val loss 2.77300 acc: 0.33418 iteration: 275\n",
      "batch 275 loss: 2.60775 acc: 0.37871 | val loss 3.57921 acc: 0.27949 iteration: 276\n",
      "batch 276 loss: 2.36528 acc: 0.44551 | val loss 2.33406 acc: 0.43096 iteration: 277\n",
      "batch 277 loss: 2.44855 acc: 0.40684 | val loss 2.51808 acc: 0.39668 iteration: 278\n",
      "batch 278 loss: 2.46560 acc: 0.41855 | val loss 2.58845 acc: 0.39297 iteration: 279\n",
      "batch 279 loss: 2.38061 acc: 0.44609 | val loss 2.25806 acc: 0.45078 iteration: 280\n",
      "batch 280 loss: 2.50978 acc: 0.42568 | val loss 2.25828 acc: 0.43838 iteration: 281\n",
      "batch 281 loss: 2.54559 acc: 0.39102 | val loss 2.19428 acc: 0.42568 iteration: 282\n",
      "batch 282 loss: 2.44653 acc: 0.40479 | val loss 2.31639 acc: 0.42939 iteration: 283\n",
      "batch 283 loss: 2.35616 acc: 0.41973 | val loss 2.67278 acc: 0.31396 iteration: 284\n",
      "batch 284 loss: 2.41204 acc: 0.41279 | val loss 1.48665 acc: 0.67324 iteration: 285\n",
      "batch 285 loss: 2.47690 acc: 0.39844 | val loss 1.81385 acc: 0.57529 iteration: 286\n",
      "batch 286 loss: 2.44161 acc: 0.41846 | val loss 2.14631 acc: 0.57910 iteration: 287\n",
      "batch 287 loss: 2.45715 acc: 0.40781 | val loss 3.00256 acc: 0.27275 iteration: 288\n",
      "batch 288 loss: 2.41445 acc: 0.41914 | val loss 2.17293 acc: 0.56318 iteration: 289\n",
      "batch 289 loss: 2.65117 acc: 0.36406 | val loss 2.12617 acc: 0.56396 iteration: 290\n",
      "batch 290 loss: 2.40983 acc: 0.40820 | val loss 2.34754 acc: 0.37393 iteration: 291\n",
      "batch 291 loss: 2.33756 acc: 0.42412 | val loss 2.22900 acc: 0.46895 iteration: 292\n",
      "batch 292 loss: 2.41886 acc: 0.40449 | val loss 2.27502 acc: 0.47900 iteration: 293\n",
      "batch 293 loss: 2.43626 acc: 0.40967 | val loss 2.42274 acc: 0.43389 iteration: 294\n",
      "batch 294 loss: 2.25748 acc: 0.44639 | val loss 2.02578 acc: 0.50410 iteration: 295\n",
      "batch 295 loss: 2.47842 acc: 0.40693 | val loss 1.93069 acc: 0.50908 iteration: 296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 296 loss: 2.32697 acc: 0.42529 | val loss 1.91049 acc: 0.61523 iteration: 297\n",
      "batch 297 loss: 2.32642 acc: 0.41123 | val loss 1.88909 acc: 0.61299 iteration: 298\n",
      "batch 298 loss: 2.30993 acc: 0.44609 | val loss 2.70473 acc: 0.39805 iteration: 299\n",
      "batch 299 loss: 2.51247 acc: 0.38877 | val loss 2.51947 acc: 0.36123 iteration: 300\n",
      "batch 300 loss: 2.42787 acc: 0.41650 | val loss 2.30934 acc: 0.34121 iteration: 301\n",
      "batch 301 loss: 2.32179 acc: 0.43018 | val loss 3.30012 acc: 0.32949 iteration: 302\n",
      "batch 302 loss: 2.31771 acc: 0.42920 | val loss 2.21789 acc: 0.42402 iteration: 303\n",
      "batch 303 loss: 2.56081 acc: 0.36729 | val loss 2.31405 acc: 0.41895 iteration: 304\n",
      "batch 304 loss: 2.47831 acc: 0.40830 | val loss 2.26271 acc: 0.46172 iteration: 305\n",
      "batch 305 loss: 2.40957 acc: 0.39912 | val loss 2.45157 acc: 0.52246 iteration: 306\n",
      "batch 306 loss: 2.34896 acc: 0.41924 | val loss 1.94645 acc: 0.52432 iteration: 307\n",
      "batch 307 loss: 2.20366 acc: 0.45020 | val loss 1.98826 acc: 0.52656 iteration: 308\n",
      "batch 308 loss: 2.23709 acc: 0.41279 | val loss 2.16011 acc: 0.47559 iteration: 309\n",
      "batch 309 loss: 2.32994 acc: 0.43389 | val loss 2.01868 acc: 0.47227 iteration: 310\n",
      "batch 310 loss: 2.50434 acc: 0.38711 | val loss 2.01629 acc: 0.46943 iteration: 311\n",
      "batch 311 loss: 2.41217 acc: 0.42764 | val loss 2.39992 acc: 0.39561 iteration: 312\n",
      "batch 312 loss: 2.42008 acc: 0.41904 | val loss 2.68490 acc: 0.32793 iteration: 313\n",
      "batch 313 loss: 2.32042 acc: 0.43516 | val loss 1.93601 acc: 0.55723 iteration: 314\n",
      "batch 314 loss: 2.42406 acc: 0.40586 | val loss 2.43363 acc: 0.40693 iteration: 315\n",
      "batch 315 loss: 2.48316 acc: 0.37598 | val loss 3.32651 acc: 0.20352 iteration: 316\n",
      "batch 316 loss: 2.36978 acc: 0.43066 | val loss 2.91290 acc: 0.24922 iteration: 317\n",
      "batch 317 loss: 2.41937 acc: 0.40078 | val loss 3.30665 acc: 0.14834 iteration: 318\n",
      "batch 318 loss: 2.24299 acc: 0.45176 | val loss 2.74671 acc: 0.36172 iteration: 319\n",
      "batch 319 loss: 2.46391 acc: 0.40820 | val loss 2.77059 acc: 0.35693 iteration: 320\n",
      "batch 320 loss: 2.41182 acc: 0.42227 | val loss 2.92181 acc: 0.31982 iteration: 321\n",
      "batch 321 loss: 2.33006 acc: 0.43135 | val loss 2.29866 acc: 0.42080 iteration: 322\n",
      "batch 322 loss: 2.44083 acc: 0.39551 | val loss 2.61210 acc: 0.39561 iteration: 323\n",
      "batch 323 loss: 2.35807 acc: 0.41406 | val loss 2.19706 acc: 0.46064 iteration: 324\n",
      "batch 324 loss: 2.33533 acc: 0.43076 | val loss 2.04469 acc: 0.47314 iteration: 325\n",
      "batch 325 loss: 2.30257 acc: 0.45977 | val loss 2.29958 acc: 0.41582 iteration: 326\n",
      "batch 326 loss: 2.35676 acc: 0.42432 | val loss 2.58331 acc: 0.33066 iteration: 327\n",
      "batch 327 loss: 2.58161 acc: 0.37148 | val loss 2.51242 acc: 0.39629 iteration: 328\n",
      "batch 328 loss: 2.32905 acc: 0.43936 | val loss 2.80568 acc: 0.32803 iteration: 329\n",
      "batch 329 loss: 2.30846 acc: 0.43965 | val loss 2.73291 acc: 0.35352 iteration: 330\n",
      "batch 330 loss: 2.24970 acc: 0.46074 | val loss 2.77388 acc: 0.34971 iteration: 331\n",
      "batch 331 loss: 2.27774 acc: 0.44082 | val loss 2.79203 acc: 0.30840 iteration: 332\n",
      "batch 332 loss: 2.34016 acc: 0.43408 | val loss 1.66330 acc: 0.53389 iteration: 333\n",
      "batch 333 loss: 2.39844 acc: 0.39717 | val loss 1.67750 acc: 0.52510 iteration: 334\n",
      "batch 334 loss: 2.28187 acc: 0.43584 | val loss 3.08887 acc: 0.29854 iteration: 335\n",
      "batch 335 loss: 2.45435 acc: 0.40986 | val loss 3.06977 acc: 0.32676 iteration: 336\n",
      "batch 336 loss: 2.30508 acc: 0.40889 | val loss 2.53666 acc: 0.43926 iteration: 337\n",
      "batch 337 loss: 2.42111 acc: 0.42100 | val loss 2.47230 acc: 0.42080 iteration: 338\n",
      "batch 338 loss: 2.32429 acc: 0.42520 | val loss 2.03969 acc: 0.50938 iteration: 339\n",
      "batch 339 loss: 2.25031 acc: 0.45010 | val loss 2.11560 acc: 0.50176 iteration: 340\n",
      "batch 340 loss: 2.48656 acc: 0.38252 | val loss 2.53516 acc: 0.37080 iteration: 341\n",
      "batch 341 loss: 2.34447 acc: 0.42607 | val loss 2.21826 acc: 0.48428 iteration: 342\n",
      "batch 342 loss: 2.47095 acc: 0.41182 | val loss 2.21669 acc: 0.48633 iteration: 343\n",
      "batch 343 loss: 2.20706 acc: 0.43770 | val loss 2.53647 acc: 0.39229 iteration: 344\n",
      "batch 344 loss: 2.23763 acc: 0.42900 | val loss 2.82307 acc: 0.25850 iteration: 345\n",
      "batch 345 loss: 2.36891 acc: 0.42393 | val loss 2.50185 acc: 0.33232 iteration: 346\n",
      "batch 346 loss: 2.39813 acc: 0.42061 | val loss 1.99405 acc: 0.53027 iteration: 347\n",
      "batch 347 loss: 2.30596 acc: 0.44023 | val loss 1.94389 acc: 0.50645 iteration: 348\n",
      "batch 348 loss: 2.34081 acc: 0.38730 | val loss 2.56268 acc: 0.33086 iteration: 349\n",
      "batch 349 loss: 2.26132 acc: 0.43486 | val loss 2.79039 acc: 0.27998 iteration: 350\n",
      "batch 350 loss: 2.48278 acc: 0.39453 | val loss 2.29839 acc: 0.38203 iteration: 351\n",
      "batch 351 loss: 2.37080 acc: 0.41982 | val loss 2.37315 acc: 0.39609 iteration: 352\n",
      "batch 352 loss: 2.31191 acc: 0.42480 | val loss 2.56715 acc: 0.37236 iteration: 353\n",
      "batch 353 loss: 2.40581 acc: 0.41572 | val loss 2.55115 acc: 0.37959 iteration: 354\n",
      "batch 354 loss: 2.28494 acc: 0.41504 | val loss 2.42241 acc: 0.38545 iteration: 355\n",
      "batch 355 loss: 2.30655 acc: 0.41768 | val loss 2.94476 acc: 0.28574 iteration: 356\n",
      "batch 356 loss: 2.36209 acc: 0.40098 | val loss 2.51461 acc: 0.39395 iteration: 357\n",
      "batch 357 loss: 2.40609 acc: 0.42031 | val loss 2.13441 acc: 0.52549 iteration: 358\n",
      "batch 358 loss: 2.45363 acc: 0.40254 | val loss 2.26998 acc: 0.45449 iteration: 359\n",
      "batch 359 loss: 2.35255 acc: 0.42803 | val loss 2.33848 acc: 0.49082 iteration: 360\n",
      "batch 360 loss: 2.32104 acc: 0.43809 | val loss 2.11281 acc: 0.58857 iteration: 361\n",
      "batch 361 loss: 2.26328 acc: 0.45273 | val loss 2.58724 acc: 0.39375 iteration: 362\n",
      "batch 362 loss: 2.35519 acc: 0.40283 | val loss 3.00247 acc: 0.28887 iteration: 363\n",
      "batch 363 loss: 2.40839 acc: 0.40586 | val loss 2.39231 acc: 0.35889 iteration: 364\n",
      "batch 364 loss: 2.38342 acc: 0.39160 | val loss 2.07003 acc: 0.48545 iteration: 365\n",
      "batch 365 loss: 2.40059 acc: 0.40518 | val loss 1.94850 acc: 0.56357 iteration: 366\n",
      "batch 366 loss: 2.28640 acc: 0.40498 | val loss 2.92440 acc: 0.29482 iteration: 367\n",
      "batch 367 loss: 2.42998 acc: 0.38818 | val loss 2.58818 acc: 0.34961 iteration: 368\n",
      "batch 368 loss: 2.32030 acc: 0.43926 | val loss 1.77294 acc: 0.50654 iteration: 369\n",
      "batch 369 loss: 2.31057 acc: 0.44141 | val loss 1.91141 acc: 0.50752 iteration: 370\n",
      "batch 370 loss: 2.35472 acc: 0.42461 | val loss 1.98499 acc: 0.52305 iteration: 371\n",
      "batch 371 loss: 2.37683 acc: 0.39883 | val loss 2.25678 acc: 0.43887 iteration: 372\n",
      "batch 372 loss: 2.39476 acc: 0.42998 | val loss 2.27464 acc: 0.38174 iteration: 373\n",
      "batch 373 loss: 2.36520 acc: 0.41650 | val loss 2.02847 acc: 0.47754 iteration: 374\n",
      "batch 374 loss: 2.41929 acc: 0.42002 | val loss 2.08487 acc: 0.46465 iteration: 375\n",
      "batch 375 loss: 2.55560 acc: 0.38086 | val loss 2.97891 acc: 0.37627 iteration: 376\n",
      "batch 376 loss: 2.27009 acc: 0.42930 | val loss 2.06135 acc: 0.52959 iteration: 377\n",
      "batch 377 loss: 2.11558 acc: 0.45039 | val loss 2.53986 acc: 0.44355 iteration: 378\n",
      "batch 378 loss: 2.42835 acc: 0.44561 | val loss 1.88849 acc: 0.55029 iteration: 379\n",
      "batch 379 loss: 2.43160 acc: 0.40293 | val loss 1.76470 acc: 0.61006 iteration: 380\n",
      "batch 380 loss: 2.33921 acc: 0.43486 | val loss 1.88828 acc: 0.52012 iteration: 381\n",
      "batch 381 loss: 2.28191 acc: 0.41680 | val loss 2.14164 acc: 0.48936 iteration: 382\n",
      "batch 382 loss: 2.23583 acc: 0.45117 | val loss 2.00273 acc: 0.48057 iteration: 383\n",
      "batch 383 loss: 2.36430 acc: 0.41768 | val loss 3.19509 acc: 0.28906 iteration: 384\n",
      "batch 384 loss: 2.07474 acc: 0.48818 | val loss 2.07664 acc: 0.51992 iteration: 385\n",
      "batch 385 loss: 2.31896 acc: 0.41826 | val loss 1.86686 acc: 0.54512 iteration: 386\n",
      "batch 386 loss: 2.26449 acc: 0.42061 | val loss 2.47663 acc: 0.37061 iteration: 387\n",
      "batch 387 loss: 2.19291 acc: 0.47451 | val loss 2.16723 acc: 0.45361 iteration: 388\n",
      "batch 388 loss: 2.35552 acc: 0.42441 | val loss 2.76970 acc: 0.31357 iteration: 389\n",
      "batch 389 loss: 2.27853 acc: 0.43623 | val loss 2.22760 acc: 0.44883 iteration: 390\n",
      "batch 390 loss: 2.30489 acc: 0.43916 | val loss 1.94355 acc: 0.51455 iteration: 391\n",
      "batch 391 loss: 2.28608 acc: 0.43418 | val loss 2.95794 acc: 0.25078 iteration: 392\n",
      "batch 392 loss: 2.16672 acc: 0.46377 | val loss 2.35057 acc: 0.37373 iteration: 393\n",
      "batch 393 loss: 2.23965 acc: 0.41670 | val loss 3.26672 acc: 0.27588 iteration: 394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 394 loss: 2.26450 acc: 0.43525 | val loss 2.15940 acc: 0.43877 iteration: 395\n",
      "batch 395 loss: 2.31173 acc: 0.42373 | val loss 2.34335 acc: 0.41738 iteration: 396\n",
      "batch 396 loss: 2.24161 acc: 0.44297 | val loss 2.29195 acc: 0.43018 iteration: 397\n",
      "batch 397 loss: 2.12979 acc: 0.45918 | val loss 1.98600 acc: 0.48398 iteration: 398\n",
      "batch 398 loss: 2.44991 acc: 0.38350 | val loss 2.00800 acc: 0.46582 iteration: 399\n",
      "batch 399 loss: 2.30069 acc: 0.41885 | val loss 2.01025 acc: 0.41670 iteration: 400\n",
      "batch 400 loss: 2.21902 acc: 0.44395 | val loss 2.13174 acc: 0.45117 iteration: 401\n",
      "batch 401 loss: 2.25638 acc: 0.43096 | val loss 2.26520 acc: 0.41055 iteration: 402\n",
      "batch 402 loss: 2.18955 acc: 0.47197 | val loss 1.26182 acc: 0.73145 iteration: 403\n",
      "batch 403 loss: 2.24338 acc: 0.43799 | val loss 1.55148 acc: 0.60791 iteration: 404\n",
      "batch 404 loss: 2.10256 acc: 0.43838 | val loss 1.88435 acc: 0.57402 iteration: 405\n",
      "batch 405 loss: 2.16881 acc: 0.44082 | val loss 2.70730 acc: 0.28945 iteration: 406\n",
      "batch 406 loss: 2.23480 acc: 0.42217 | val loss 2.09065 acc: 0.55645 iteration: 407\n",
      "batch 407 loss: 2.28658 acc: 0.42764 | val loss 2.00586 acc: 0.58818 iteration: 408\n",
      "batch 408 loss: 2.16495 acc: 0.42188 | val loss 2.24852 acc: 0.35459 iteration: 409\n",
      "batch 409 loss: 2.28724 acc: 0.43154 | val loss 2.14681 acc: 0.47891 iteration: 410\n",
      "batch 410 loss: 2.27767 acc: 0.42119 | val loss 2.15813 acc: 0.47969 iteration: 411\n",
      "batch 411 loss: 2.17908 acc: 0.43721 | val loss 2.22523 acc: 0.44082 iteration: 412\n",
      "batch 412 loss: 2.30601 acc: 0.42344 | val loss 1.87684 acc: 0.53047 iteration: 413\n",
      "batch 413 loss: 2.15215 acc: 0.44072 | val loss 1.75158 acc: 0.54639 iteration: 414\n",
      "batch 414 loss: 2.19717 acc: 0.43848 | val loss 1.61807 acc: 0.61934 iteration: 415\n",
      "batch 415 loss: 2.16951 acc: 0.43818 | val loss 1.65921 acc: 0.60508 iteration: 416\n",
      "batch 416 loss: 2.17707 acc: 0.43213 | val loss 2.44798 acc: 0.38115 iteration: 417\n",
      "batch 417 loss: 2.24404 acc: 0.43691 | val loss 2.33738 acc: 0.37510 iteration: 418\n",
      "batch 418 loss: 2.31385 acc: 0.42969 | val loss 2.22325 acc: 0.39121 iteration: 419\n",
      "batch 419 loss: 2.01437 acc: 0.46738 | val loss 3.21719 acc: 0.34229 iteration: 420\n",
      "batch 420 loss: 2.24840 acc: 0.41465 | val loss 2.08881 acc: 0.44766 iteration: 421\n",
      "batch 421 loss: 2.08710 acc: 0.47529 | val loss 2.11576 acc: 0.44219 iteration: 422\n",
      "batch 422 loss: 2.33643 acc: 0.42920 | val loss 2.16568 acc: 0.45732 iteration: 423\n",
      "batch 423 loss: 2.14924 acc: 0.45186 | val loss 2.30199 acc: 0.51250 iteration: 424\n",
      "batch 424 loss: 2.17102 acc: 0.44932 | val loss 1.85647 acc: 0.52090 iteration: 425\n",
      "batch 425 loss: 2.12906 acc: 0.44922 | val loss 1.94458 acc: 0.53936 iteration: 426\n",
      "batch 426 loss: 2.26628 acc: 0.42168 | val loss 2.06771 acc: 0.47471 iteration: 427\n",
      "batch 427 loss: 2.40084 acc: 0.41582 | val loss 1.81845 acc: 0.48018 iteration: 428\n",
      "batch 428 loss: 2.17462 acc: 0.44502 | val loss 1.78500 acc: 0.47754 iteration: 429\n",
      "batch 429 loss: 2.00612 acc: 0.47061 | val loss 2.14022 acc: 0.41152 iteration: 430\n",
      "batch 430 loss: 2.13235 acc: 0.45811 | val loss 2.56874 acc: 0.35459 iteration: 431\n",
      "batch 431 loss: 2.31003 acc: 0.40225 | val loss 1.90349 acc: 0.55527 iteration: 432\n",
      "batch 432 loss: 2.16993 acc: 0.44502 | val loss 2.35679 acc: 0.41826 iteration: 433\n",
      "batch 433 loss: 2.13269 acc: 0.46670 | val loss 3.05144 acc: 0.21582 iteration: 434\n",
      "batch 434 loss: 2.07205 acc: 0.47754 | val loss 2.72645 acc: 0.26826 iteration: 435\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "logs = []\n",
    "device = torch.device(torch.cuda.current_device())\n",
    "iteration = 0\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch: %d\\n__________________________________________\" % (epoch), flush = True)\n",
    "    mean_loss = 0.0\n",
    "    mean_acc = 0.0\n",
    "    v_mean_loss = 0.0\n",
    "    v_mean_acc = 0.0\n",
    "    total = 0\n",
    "    for i, d in enumerate(train_set):\n",
    "        # validate display\n",
    "        x = gd = d[0]\n",
    "        model.train()\n",
    "        j = i % len(validate_data)\n",
    "        v_x = v_gd = validate_data[j]\n",
    "        \n",
    "        x = x.to(device = device,non_blocking = True)\n",
    "        gd = gd.to(device = device,non_blocking = True)\n",
    "        v_x = v_x.to(device = device,non_blocking = True)\n",
    "        v_gd = v_gd.to(device = device,non_blocking = True)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        recon, r_dis, iteration = model(x, gd)\n",
    "        \n",
    "        acc, loss = loss_function(recon, gd.view(-1), r_dis, vae_beta)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.item()\n",
    "        mean_acc += acc.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_recon, v_r_dis, _ = model(v_x, v_gd)\n",
    "            v_acc, v_loss = loss_function(v_recon, v_gd.view(-1), v_r_dis, vae_beta)\n",
    "            v_mean_loss += v_loss.item()\n",
    "            v_mean_acc += v_acc.item()\n",
    "        step += 1\n",
    "        total += 1\n",
    "        if decay > 0:\n",
    "            scheduler.step()\n",
    "        print(\"batch %d loss: %.5f acc: %.5f | val loss %.5f acc: %.5f iteration: %d\"  \n",
    "              % (i,loss.item(), acc.item(), v_loss.item(),v_acc.item(),iteration),flush = True)\n",
    "    mean_loss /= total\n",
    "    mean_acc /= total\n",
    "    v_mean_loss /= total\n",
    "    v_mean_acc /= total\n",
    "    print(\"epoch %d loss: %.5f acc: %.5f | val loss %.5f acc: %.5f iteration: %d\"  \n",
    "              % (epoch, mean_loss, mean_acc, v_mean_loss, v_mean_acc, iteration),flush = True)\n",
    "    logs.append([mean_loss,mean_acc,v_mean_loss,v_mean_acc,iteration])\n",
    "    if (epoch + 1) % save_period == 0:\n",
    "        filename = \"sketchvae-\" + 'loss_' + str(mean_loss) + \"_\" + str(epoch+1) + \"_\" + str(iteration) + \".pt\"\n",
    "        torch.save(model.cpu().state_dict(),save_path + filename)\n",
    "        model.cuda()\n",
    "    np.save(\"sketchvae-log.npy\", logs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
