{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this file \n",
    "# we train our proposed PolyVAE\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.distributions import kl_divergence, Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from model import PolyVAE\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class MinExponentialLR(ExponentialLR):\n",
    "    def __init__(self, optimizer, gamma, minimum, last_epoch=-1):\n",
    "        self.min = minimum\n",
    "        super(MinExponentialLR, self).__init__(optimizer, gamma, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "###############################\n",
    "# initial parameters\n",
    "s_dir = \"\"\n",
    "batch_size = 64\n",
    "n_epochs = 4\n",
    "data_path = [s_dir + \"data/poly_train.npy\",\n",
    "             s_dir + \"data/poly_validate.npy\",\n",
    "             s_dir + \"data/poly_train.npy\"]\n",
    "save_path = \"\"\n",
    "lr = 1e-4\n",
    "decay = 0.9999\n",
    "hidden_dims = 512\n",
    "z_dims = 1024\n",
    "vae_beta = 0.1\n",
    "input_dims = 130\n",
    "seq_len = 10 * 16\n",
    "beat_num = 10\n",
    "tick_num = 16\n",
    "save_period = 1\n",
    "experiment_name = \"default\"\n",
    "##############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([58041, 160])\ntorch.Size([7578, 160])\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "train_set = np.load(data_path[0], allow_pickle = True)\n",
    "validate_set = np.load(data_path[1],allow_pickle = True) \n",
    "\n",
    "train_x = []\n",
    "for i,data in enumerate(train_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    train_x.append(temp)\n",
    "train_x = np.array(train_x)\n",
    "# print(train_x.shape)\n",
    "\n",
    "validate_x = []\n",
    "for i,data in enumerate(validate_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    validate_x.append(temp)\n",
    "validate_x = np.array(validate_x)\n",
    "# print(train_x.shape)\n",
    "train_x = torch.from_numpy(train_x).long()\n",
    "validate_x = torch.from_numpy(validate_x).long()\n",
    "\n",
    "print(train_x.size())\n",
    "print(validate_x.size())\n",
    "\n",
    "train_set = TensorDataset(train_x)\n",
    "validate_set = TensorDataset(validate_x)\n",
    "\n",
    "train_set = DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "validate_set = DataLoader(\n",
    "    dataset = validate_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using:  TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "model = PolyVAE(input_dims, hidden_dims, z_dims, seq_len, beat_num, tick_num, 4000)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "if decay > 0:\n",
    "    scheduler = MinExponentialLR(optimizer, gamma = decay, minimum = 1e-5)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using: CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "# process validete data from the dataloder\n",
    "validate_data = []\n",
    "for i,d in enumerate(validate_set):\n",
    "    validate_data.append(d[0])\n",
    "print(len(validate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def std_normal(shape):\n",
    "    N = Normal(torch.zeros(shape), torch.ones(shape))\n",
    "    if torch.cuda.is_available():\n",
    "        N.loc = N.loc.cuda()\n",
    "        N.scale = N.scale.cuda()\n",
    "    return N\n",
    "\n",
    "def loss_function(recon, target, r_dis, beta):\n",
    "    CE = F.cross_entropy(recon.view(-1, recon.size(-1)), target, reduction = \"mean\")\n",
    "#     rhy_CE = F.nll_loss(recon_rhythm.view(-1, recon_rhythm.size(-1)), target_rhythm, reduction = \"mean\")\n",
    "    normal1 =  std_normal(r_dis.mean.size())\n",
    "    KLD1 = kl_divergence(r_dis, normal1).mean()\n",
    "    max_indices = recon.view(-1, recon.size(-1)).max(-1)[-1]\n",
    "#     print(max_indices)\n",
    "    correct = max_indices == target\n",
    "    acc = torch.sum(correct.float()) / target.size(0)\n",
    "    return acc, CE + beta * (KLD1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer):\n",
    "    root_model_path = os.path.join(experiment_dir, 'best_model.pt')\n",
    "    model_dict = model.state_dict()\n",
    "    state_dict = {'model': model_dict, 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state_dict, root_model_path)\n",
    "\n",
    "def load_model(model, optimizer):\n",
    "    state_dict = torch.load(os.path.join(experiment_dir, 'best_model.pt'))\n",
    "    model.load_state_dict(state_dict['model'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer'])\n",
    "\n",
    "def record_stats(train_loss, train_acc, val_loss, val_acc):\n",
    "    training_losses.append(train_loss)\n",
    "    training_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    plot_stats()\n",
    "\n",
    "def plot_stats():\n",
    "    e = len(training_losses)\n",
    "    x_axis = np.arange(1, e + 1, 1)\n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, training_losses, label=\"Training Loss\")\n",
    "    plt.plot(x_axis, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(os.path.join(experiment_dir, \"loss_plot.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x_axis, training_accs, label=\"Training Accuracy\")\n",
    "    plt.plot(x_axis, val_accs, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(os.path.join(experiment_dir, \"acc_plot.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c: 0.35557 iteration: 1578\n",
      "batch 672 loss: 1.58351 acc: 0.55801 | val loss 2.14852 acc: 0.41904 iteration: 1579\n",
      "batch 673 loss: 1.42782 acc: 0.58779 | val loss 2.04590 acc: 0.43506 iteration: 1580\n",
      "batch 674 loss: 1.48352 acc: 0.57686 | val loss 1.97702 acc: 0.38203 iteration: 1581\n",
      "batch 675 loss: 1.56484 acc: 0.56299 | val loss 1.45002 acc: 0.56465 iteration: 1582\n",
      "batch 676 loss: 1.38614 acc: 0.60635 | val loss 1.86669 acc: 0.47656 iteration: 1583\n",
      "batch 677 loss: 1.56394 acc: 0.54980 | val loss 1.60041 acc: 0.53145 iteration: 1584\n",
      "batch 678 loss: 1.38407 acc: 0.62207 | val loss 1.51593 acc: 0.53691 iteration: 1585\n",
      "batch 679 loss: 1.45253 acc: 0.57129 | val loss 1.56095 acc: 0.53760 iteration: 1586\n",
      "batch 680 loss: 1.47279 acc: 0.56689 | val loss 1.70205 acc: 0.49492 iteration: 1587\n",
      "batch 681 loss: 1.47808 acc: 0.59658 | val loss 1.89621 acc: 0.47598 iteration: 1588\n",
      "batch 682 loss: 1.44367 acc: 0.58730 | val loss 2.12022 acc: 0.41113 iteration: 1589\n",
      "batch 683 loss: 1.53246 acc: 0.55088 | val loss 2.05683 acc: 0.43281 iteration: 1590\n",
      "batch 684 loss: 1.44042 acc: 0.58076 | val loss 1.82351 acc: 0.49238 iteration: 1591\n",
      "batch 685 loss: 1.41085 acc: 0.58428 | val loss 1.56161 acc: 0.49453 iteration: 1592\n",
      "batch 686 loss: 1.48445 acc: 0.57217 | val loss 1.12392 acc: 0.65049 iteration: 1593\n",
      "batch 687 loss: 1.62133 acc: 0.53730 | val loss 1.10642 acc: 0.65371 iteration: 1594\n",
      "batch 688 loss: 1.60495 acc: 0.54063 | val loss 1.50357 acc: 0.53311 iteration: 1595\n",
      "batch 689 loss: 1.51033 acc: 0.55479 | val loss 1.59329 acc: 0.52764 iteration: 1596\n",
      "batch 690 loss: 1.44062 acc: 0.58838 | val loss 1.64813 acc: 0.55625 iteration: 1597\n",
      "batch 691 loss: 1.59774 acc: 0.54746 | val loss 1.79586 acc: 0.51162 iteration: 1598\n",
      "batch 692 loss: 1.56246 acc: 0.55479 | val loss 1.45408 acc: 0.59736 iteration: 1599\n",
      "batch 693 loss: 1.53689 acc: 0.54883 | val loss 1.50210 acc: 0.58848 iteration: 1600\n",
      "batch 694 loss: 1.53754 acc: 0.55293 | val loss 1.69980 acc: 0.50869 iteration: 1601\n",
      "batch 695 loss: 1.57171 acc: 0.54756 | val loss 1.73398 acc: 0.54902 iteration: 1602\n",
      "batch 696 loss: 1.44801 acc: 0.57598 | val loss 1.75415 acc: 0.55605 iteration: 1603\n",
      "batch 697 loss: 1.42913 acc: 0.58525 | val loss 1.81379 acc: 0.51904 iteration: 1604\n",
      "batch 698 loss: 1.52464 acc: 0.55146 | val loss 1.80356 acc: 0.45547 iteration: 1605\n",
      "batch 699 loss: 1.51555 acc: 0.56846 | val loss 1.47024 acc: 0.52188 iteration: 1606\n",
      "batch 700 loss: 1.45672 acc: 0.56982 | val loss 1.33011 acc: 0.63311 iteration: 1607\n",
      "batch 701 loss: 1.35912 acc: 0.60791 | val loss 1.12112 acc: 0.64844 iteration: 1608\n",
      "batch 702 loss: 1.58016 acc: 0.54033 | val loss 1.84894 acc: 0.47188 iteration: 1609\n",
      "batch 703 loss: 1.49432 acc: 0.55801 | val loss 1.88258 acc: 0.44395 iteration: 1610\n",
      "batch 704 loss: 1.38572 acc: 0.59365 | val loss 1.25903 acc: 0.60664 iteration: 1611\n",
      "batch 705 loss: 1.46939 acc: 0.55371 | val loss 1.20806 acc: 0.58936 iteration: 1612\n",
      "batch 706 loss: 1.40673 acc: 0.56943 | val loss 1.61623 acc: 0.53877 iteration: 1613\n",
      "batch 707 loss: 1.49945 acc: 0.57783 | val loss 1.91206 acc: 0.49541 iteration: 1614\n",
      "batch 708 loss: 1.47988 acc: 0.57793 | val loss 1.88213 acc: 0.49336 iteration: 1615\n",
      "batch 709 loss: 1.63753 acc: 0.52441 | val loss 2.22136 acc: 0.39111 iteration: 1616\n",
      "batch 710 loss: 1.47128 acc: 0.56240 | val loss 1.68387 acc: 0.52012 iteration: 1617\n",
      "batch 711 loss: 1.42130 acc: 0.58936 | val loss 1.26183 acc: 0.63115 iteration: 1618\n",
      "batch 712 loss: 1.34514 acc: 0.60762 | val loss 1.72457 acc: 0.49375 iteration: 1619\n",
      "batch 713 loss: 1.50243 acc: 0.56475 | val loss 1.69427 acc: 0.52266 iteration: 1620\n",
      "batch 714 loss: 1.49570 acc: 0.55566 | val loss 1.06825 acc: 0.69043 iteration: 1621\n",
      "batch 715 loss: 1.56873 acc: 0.55986 | val loss 1.68177 acc: 0.50303 iteration: 1622\n",
      "batch 716 loss: 1.42802 acc: 0.58164 | val loss 1.83085 acc: 0.47764 iteration: 1623\n",
      "batch 717 loss: 1.55224 acc: 0.55488 | val loss 1.63901 acc: 0.51455 iteration: 1624\n",
      "batch 718 loss: 1.53877 acc: 0.57783 | val loss 1.44007 acc: 0.60479 iteration: 1625\n",
      "batch 719 loss: 1.53707 acc: 0.55957 | val loss 1.51024 acc: 0.61904 iteration: 1626\n",
      "batch 720 loss: 1.40724 acc: 0.59121 | val loss 2.08390 acc: 0.40957 iteration: 1627\n",
      "batch 721 loss: 1.49496 acc: 0.56836 | val loss 1.61037 acc: 0.48721 iteration: 1628\n",
      "batch 722 loss: 1.54650 acc: 0.56035 | val loss 1.15630 acc: 0.67100 iteration: 1629\n",
      "batch 723 loss: 1.50061 acc: 0.56172 | val loss 1.30554 acc: 0.62959 iteration: 1630\n",
      "batch 724 loss: 1.34284 acc: 0.61162 | val loss 1.13874 acc: 0.68447 iteration: 1631\n",
      "batch 725 loss: 1.38928 acc: 0.59287 | val loss 1.45387 acc: 0.58477 iteration: 1632\n",
      "batch 726 loss: 1.74157 acc: 0.51943 | val loss 1.65558 acc: 0.51973 iteration: 1633\n",
      "batch 727 loss: 1.45864 acc: 0.59062 | val loss 1.50289 acc: 0.56455 iteration: 1634\n",
      "batch 728 loss: 1.40783 acc: 0.58350 | val loss 1.49506 acc: 0.57920 iteration: 1635\n",
      "batch 729 loss: 1.57817 acc: 0.55518 | val loss 2.60063 acc: 0.51211 iteration: 1636\n",
      "batch 730 loss: 1.52568 acc: 0.55215 | val loss 1.34204 acc: 0.61826 iteration: 1637\n",
      "batch 731 loss: 1.53767 acc: 0.55518 | val loss 1.84179 acc: 0.50137 iteration: 1638\n",
      "batch 732 loss: 1.60027 acc: 0.56543 | val loss 1.44328 acc: 0.59766 iteration: 1639\n",
      "batch 733 loss: 1.64902 acc: 0.52773 | val loss 1.42493 acc: 0.61357 iteration: 1640\n",
      "batch 734 loss: 1.45733 acc: 0.57764 | val loss 1.28099 acc: 0.63760 iteration: 1641\n",
      "batch 735 loss: 1.47469 acc: 0.56602 | val loss 1.50274 acc: 0.60146 iteration: 1642\n",
      "batch 736 loss: 1.36058 acc: 0.62031 | val loss 1.17736 acc: 0.66563 iteration: 1643\n",
      "batch 737 loss: 1.53207 acc: 0.55557 | val loss 2.02640 acc: 0.39990 iteration: 1644\n",
      "batch 738 loss: 1.47877 acc: 0.55488 | val loss 1.49432 acc: 0.58242 iteration: 1645\n",
      "batch 739 loss: 1.43843 acc: 0.57637 | val loss 1.22255 acc: 0.63125 iteration: 1646\n",
      "batch 740 loss: 1.42605 acc: 0.59082 | val loss 1.55281 acc: 0.52148 iteration: 1647\n",
      "batch 741 loss: 1.67367 acc: 0.50879 | val loss 1.59240 acc: 0.53125 iteration: 1648\n",
      "batch 742 loss: 1.47843 acc: 0.58066 | val loss 2.14721 acc: 0.40762 iteration: 1649\n",
      "batch 743 loss: 1.40411 acc: 0.58594 | val loss 1.54229 acc: 0.56221 iteration: 1650\n",
      "batch 744 loss: 1.59421 acc: 0.57236 | val loss 1.19937 acc: 0.64375 iteration: 1651\n",
      "batch 745 loss: 1.46355 acc: 0.56426 | val loss 1.64299 acc: 0.44580 iteration: 1652\n",
      "batch 746 loss: 1.34427 acc: 0.59326 | val loss 1.73645 acc: 0.48877 iteration: 1653\n",
      "batch 747 loss: 1.48707 acc: 0.57549 | val loss 2.34514 acc: 0.38887 iteration: 1654\n",
      "batch 748 loss: 1.38194 acc: 0.60996 | val loss 1.37217 acc: 0.58975 iteration: 1655\n",
      "batch 749 loss: 1.45023 acc: 0.55801 | val loss 1.53360 acc: 0.55205 iteration: 1656\n",
      "batch 750 loss: 1.46702 acc: 0.56377 | val loss 1.45807 acc: 0.56602 iteration: 1657\n",
      "batch 751 loss: 1.43126 acc: 0.59561 | val loss 1.12093 acc: 0.67588 iteration: 1658\n",
      "batch 752 loss: 1.40306 acc: 0.60342 | val loss 1.46331 acc: 0.58564 iteration: 1659\n",
      "batch 753 loss: 1.36077 acc: 0.60859 | val loss 1.41671 acc: 0.53242 iteration: 1660\n",
      "batch 754 loss: 1.40589 acc: 0.58613 | val loss 1.57825 acc: 0.54951 iteration: 1661\n",
      "batch 755 loss: 1.44023 acc: 0.58955 | val loss 1.43365 acc: 0.56084 iteration: 1662\n",
      "batch 756 loss: 1.51149 acc: 0.56582 | val loss 0.94833 acc: 0.76240 iteration: 1663\n",
      "batch 757 loss: 1.47402 acc: 0.58281 | val loss 1.05748 acc: 0.70674 iteration: 1664\n",
      "batch 758 loss: 1.42632 acc: 0.57998 | val loss 1.06220 acc: 0.68760 iteration: 1665\n",
      "batch 759 loss: 1.47953 acc: 0.57891 | val loss 2.02540 acc: 0.41289 iteration: 1666\n",
      "batch 760 loss: 1.61462 acc: 0.53711 | val loss 1.83117 acc: 0.52939 iteration: 1667\n",
      "batch 761 loss: 1.63327 acc: 0.54932 | val loss 1.63079 acc: 0.59180 iteration: 1668\n",
      "batch 762 loss: 1.52823 acc: 0.54941 | val loss 1.49652 acc: 0.52686 iteration: 1669\n",
      "batch 763 loss: 1.42093 acc: 0.60352 | val loss 1.40189 acc: 0.58105 iteration: 1670\n",
      "batch 764 loss: 1.43225 acc: 0.57715 | val loss 1.39240 acc: 0.59521 iteration: 1671\n",
      "batch 765 loss: 1.52954 acc: 0.54619 | val loss 1.66178 acc: 0.51113 iteration: 1672\n",
      "batch 766 loss: 1.51504 acc: 0.55498 | val loss 1.28932 acc: 0.61680 iteration: 1673\n",
      "batch 767 loss: 1.49765 acc: 0.55371 | val loss 1.09285 acc: 0.67275 iteration: 1674\n",
      "batch 768 loss: 1.44300 acc: 0.58174 | val loss 1.21629 acc: 0.62891 iteration: 1675\n",
      "batch 769 loss: 1.45958 acc: 0.58398 | val loss 1.30826 acc: 0.59678 iteration: 1676\n",
      "batch 770 loss: 1.46552 acc: 0.58252 | val loss 1.58871 acc: 0.50986 iteration: 1677\n",
      "batch 771 loss: 1.48863 acc: 0.56309 | val loss 1.65537 acc: 0.47002 iteration: 1678\n",
      "batch 772 loss: 1.45730 acc: 0.57119 | val loss 1.61130 acc: 0.49834 iteration: 1679\n",
      "batch 773 loss: 1.55903 acc: 0.54639 | val loss 2.36834 acc: 0.39824 iteration: 1680\n",
      "batch 774 loss: 1.36154 acc: 0.60645 | val loss 1.52182 acc: 0.52715 iteration: 1681\n",
      "batch 775 loss: 1.50784 acc: 0.56357 | val loss 1.30741 acc: 0.61299 iteration: 1682\n",
      "batch 776 loss: 1.49383 acc: 0.56992 | val loss 1.60899 acc: 0.55479 iteration: 1683\n",
      "batch 777 loss: 1.53728 acc: 0.55439 | val loss 2.25099 acc: 0.46641 iteration: 1684\n",
      "batch 778 loss: 1.60366 acc: 0.54834 | val loss 1.27962 acc: 0.63838 iteration: 1685\n",
      "batch 779 loss: 1.49064 acc: 0.55361 | val loss 1.41790 acc: 0.63037 iteration: 1686\n",
      "batch 780 loss: 1.31167 acc: 0.60088 | val loss 1.35371 acc: 0.59932 iteration: 1687\n",
      "batch 781 loss: 1.47451 acc: 0.55908 | val loss 1.10805 acc: 0.63613 iteration: 1688\n",
      "batch 782 loss: 1.44655 acc: 0.59336 | val loss 1.15318 acc: 0.61787 iteration: 1689\n",
      "batch 783 loss: 1.45939 acc: 0.56270 | val loss 1.41500 acc: 0.57812 iteration: 1690\n",
      "batch 784 loss: 1.36588 acc: 0.59609 | val loss 1.95217 acc: 0.48682 iteration: 1691\n",
      "batch 785 loss: 1.51382 acc: 0.56455 | val loss 1.29706 acc: 0.64355 iteration: 1692\n",
      "batch 786 loss: 1.51512 acc: 0.54736 | val loss 1.57536 acc: 0.55010 iteration: 1693\n",
      "batch 787 loss: 1.33315 acc: 0.61514 | val loss 1.94952 acc: 0.42344 iteration: 1694\n",
      "batch 788 loss: 1.49252 acc: 0.58486 | val loss 1.74338 acc: 0.49336 iteration: 1695\n",
      "batch 789 loss: 1.41777 acc: 0.57393 | val loss 2.09770 acc: 0.36719 iteration: 1696\n",
      "batch 790 loss: 1.44393 acc: 0.58672 | val loss 2.13625 acc: 0.42822 iteration: 1697\n",
      "batch 791 loss: 1.51248 acc: 0.56777 | val loss 1.99084 acc: 0.43447 iteration: 1698\n",
      "batch 792 loss: 1.46565 acc: 0.57158 | val loss 1.92375 acc: 0.39150 iteration: 1699\n",
      "batch 793 loss: 1.39158 acc: 0.59072 | val loss 1.40359 acc: 0.57061 iteration: 1700\n",
      "batch 794 loss: 1.47294 acc: 0.56709 | val loss 1.86847 acc: 0.48311 iteration: 1701\n",
      "batch 795 loss: 1.67377 acc: 0.53242 | val loss 1.52338 acc: 0.55752 iteration: 1702\n",
      "batch 796 loss: 1.45876 acc: 0.56387 | val loss 1.45232 acc: 0.54336 iteration: 1703\n",
      "batch 797 loss: 1.58038 acc: 0.57148 | val loss 1.57541 acc: 0.53154 iteration: 1704\n",
      "batch 798 loss: 1.50982 acc: 0.56074 | val loss 1.74358 acc: 0.48691 iteration: 1705\n",
      "batch 799 loss: 1.30107 acc: 0.61670 | val loss 1.91666 acc: 0.47539 iteration: 1706\n",
      "batch 800 loss: 1.51863 acc: 0.55078 | val loss 2.08940 acc: 0.41943 iteration: 1707\n",
      "batch 801 loss: 1.55195 acc: 0.55234 | val loss 2.05425 acc: 0.42578 iteration: 1708\n",
      "batch 802 loss: 1.38398 acc: 0.59580 | val loss 1.80558 acc: 0.49287 iteration: 1709\n",
      "batch 803 loss: 1.41356 acc: 0.57979 | val loss 1.52134 acc: 0.50654 iteration: 1710\n",
      "batch 804 loss: 1.55044 acc: 0.56211 | val loss 1.05716 acc: 0.67090 iteration: 1711\n",
      "batch 805 loss: 1.58249 acc: 0.55430 | val loss 1.08186 acc: 0.65898 iteration: 1712\n",
      "batch 806 loss: 1.32671 acc: 0.61484 | val loss 1.43831 acc: 0.56602 iteration: 1713\n",
      "batch 807 loss: 1.35143 acc: 0.58779 | val loss 1.50988 acc: 0.55566 iteration: 1714\n",
      "batch 808 loss: 1.58868 acc: 0.55078 | val loss 1.61629 acc: 0.54961 iteration: 1715\n",
      "batch 809 loss: 1.40972 acc: 0.59971 | val loss 1.72990 acc: 0.51621 iteration: 1716\n",
      "batch 810 loss: 1.41598 acc: 0.59619 | val loss 1.40007 acc: 0.60752 iteration: 1717\n",
      "batch 811 loss: 1.48032 acc: 0.56992 | val loss 1.45124 acc: 0.61113 iteration: 1718\n",
      "batch 812 loss: 1.48081 acc: 0.56436 | val loss 1.66552 acc: 0.50635 iteration: 1719\n",
      "batch 813 loss: 1.33950 acc: 0.60479 | val loss 1.76385 acc: 0.54688 iteration: 1720\n",
      "batch 814 loss: 1.55594 acc: 0.54707 | val loss 1.76166 acc: 0.54600 iteration: 1721\n",
      "batch 815 loss: 1.47734 acc: 0.57559 | val loss 1.79971 acc: 0.53438 iteration: 1722\n",
      "batch 816 loss: 1.58932 acc: 0.54824 | val loss 1.79387 acc: 0.45195 iteration: 1723\n",
      "batch 817 loss: 1.51027 acc: 0.55449 | val loss 1.43001 acc: 0.53477 iteration: 1724\n",
      "batch 818 loss: 1.58555 acc: 0.56367 | val loss 1.33646 acc: 0.62656 iteration: 1725\n",
      "batch 819 loss: 1.42415 acc: 0.59229 | val loss 1.12020 acc: 0.65234 iteration: 1726\n",
      "batch 820 loss: 1.43556 acc: 0.57725 | val loss 1.85228 acc: 0.47168 iteration: 1727\n",
      "batch 821 loss: 1.48683 acc: 0.56318 | val loss 1.86846 acc: 0.46064 iteration: 1728\n",
      "batch 822 loss: 1.33919 acc: 0.59834 | val loss 1.19882 acc: 0.61572 iteration: 1729\n",
      "batch 823 loss: 1.45223 acc: 0.58027 | val loss 1.18528 acc: 0.59443 iteration: 1730\n",
      "batch 824 loss: 1.46592 acc: 0.57959 | val loss 1.60172 acc: 0.53711 iteration: 1731\n",
      "batch 825 loss: 1.51479 acc: 0.56230 | val loss 1.88294 acc: 0.49658 iteration: 1732\n",
      "batch 826 loss: 1.50158 acc: 0.55781 | val loss 1.82515 acc: 0.51719 iteration: 1733\n",
      "batch 827 loss: 1.44349 acc: 0.58633 | val loss 2.13282 acc: 0.39668 iteration: 1734\n",
      "batch 828 loss: 1.43772 acc: 0.57432 | val loss 1.66962 acc: 0.51387 iteration: 1735\n",
      "batch 829 loss: 1.41800 acc: 0.58193 | val loss 1.25220 acc: 0.63174 iteration: 1736\n",
      "batch 830 loss: 1.43563 acc: 0.58096 | val loss 1.64536 acc: 0.50996 iteration: 1737\n",
      "batch 831 loss: 1.39325 acc: 0.59941 | val loss 1.64037 acc: 0.53330 iteration: 1738\n",
      "batch 832 loss: 1.34096 acc: 0.61689 | val loss 1.06826 acc: 0.69141 iteration: 1739\n",
      "batch 833 loss: 1.37045 acc: 0.58945 | val loss 1.65787 acc: 0.50264 iteration: 1740\n",
      "batch 834 loss: 1.46239 acc: 0.57471 | val loss 1.79514 acc: 0.47813 iteration: 1741\n",
      "batch 835 loss: 1.42847 acc: 0.57295 | val loss 1.58890 acc: 0.52480 iteration: 1742\n",
      "batch 836 loss: 1.34409 acc: 0.60342 | val loss 1.39371 acc: 0.61191 iteration: 1743\n",
      "batch 837 loss: 1.45382 acc: 0.57432 | val loss 1.45519 acc: 0.63467 iteration: 1744\n",
      "batch 838 loss: 1.61376 acc: 0.54668 | val loss 2.04926 acc: 0.42197 iteration: 1745\n",
      "batch 839 loss: 1.27472 acc: 0.62432 | val loss 1.58849 acc: 0.51162 iteration: 1746\n",
      "batch 840 loss: 1.48093 acc: 0.57686 | val loss 1.11052 acc: 0.68975 iteration: 1747\n",
      "batch 841 loss: 1.42719 acc: 0.58643 | val loss 1.29138 acc: 0.62725 iteration: 1748\n",
      "batch 842 loss: 1.35129 acc: 0.60732 | val loss 1.10682 acc: 0.68486 iteration: 1749\n",
      "batch 843 loss: 1.44368 acc: 0.58584 | val loss 1.40453 acc: 0.60625 iteration: 1750\n",
      "batch 844 loss: 1.54105 acc: 0.56074 | val loss 1.61493 acc: 0.52871 iteration: 1751\n",
      "batch 845 loss: 1.60812 acc: 0.56445 | val loss 1.48722 acc: 0.56768 iteration: 1752\n",
      "batch 846 loss: 1.51575 acc: 0.56729 | val loss 1.45088 acc: 0.58770 iteration: 1753\n",
      "batch 847 loss: 1.48086 acc: 0.57646 | val loss 2.49201 acc: 0.52627 iteration: 1754\n",
      "batch 848 loss: 1.37515 acc: 0.58555 | val loss 1.31427 acc: 0.63242 iteration: 1755\n",
      "batch 849 loss: 1.45023 acc: 0.57891 | val loss 1.81870 acc: 0.50908 iteration: 1756\n",
      "batch 850 loss: 1.34924 acc: 0.60635 | val loss 1.39258 acc: 0.61641 iteration: 1757\n",
      "batch 851 loss: 1.30535 acc: 0.62002 | val loss 1.38506 acc: 0.62363 iteration: 1758\n",
      "batch 852 loss: 1.40374 acc: 0.59531 | val loss 1.26332 acc: 0.64092 iteration: 1759\n",
      "batch 853 loss: 1.55423 acc: 0.54268 | val loss 1.52617 acc: 0.58701 iteration: 1760\n",
      "batch 854 loss: 1.54248 acc: 0.54746 | val loss 1.12377 acc: 0.67314 iteration: 1761\n",
      "batch 855 loss: 1.49880 acc: 0.57402 | val loss 1.95418 acc: 0.40527 iteration: 1762\n",
      "batch 856 loss: 1.47333 acc: 0.57002 | val loss 1.42737 acc: 0.58750 iteration: 1763\n",
      "batch 857 loss: 1.47666 acc: 0.58750 | val loss 1.18253 acc: 0.63262 iteration: 1764\n",
      "batch 858 loss: 1.46689 acc: 0.57900 | val loss 1.50053 acc: 0.53828 iteration: 1765\n",
      "batch 859 loss: 1.48202 acc: 0.59170 | val loss 1.54712 acc: 0.53350 iteration: 1766\n",
      "batch 860 loss: 1.34885 acc: 0.59277 | val loss 2.12108 acc: 0.41758 iteration: 1767\n",
      "batch 861 loss: 1.41541 acc: 0.60234 | val loss 1.53627 acc: 0.56055 iteration: 1768\n",
      "batch 862 loss: 1.38500 acc: 0.59873 | val loss 1.17147 acc: 0.65527 iteration: 1769\n",
      "batch 863 loss: 1.48865 acc: 0.54541 | val loss 1.58511 acc: 0.46318 iteration: 1770\n",
      "batch 864 loss: 1.26720 acc: 0.63789 | val loss 1.67188 acc: 0.49688 iteration: 1771\n",
      "batch 865 loss: 1.51264 acc: 0.55918 | val loss 2.23162 acc: 0.42012 iteration: 1772\n",
      "batch 866 loss: 1.59178 acc: 0.55811 | val loss 1.36921 acc: 0.59492 iteration: 1773\n",
      "batch 867 loss: 1.45034 acc: 0.56699 | val loss 1.47729 acc: 0.57314 iteration: 1774\n",
      "batch 868 loss: 1.54677 acc: 0.56387 | val loss 1.45982 acc: 0.56689 iteration: 1775\n",
      "batch 869 loss: 1.36434 acc: 0.60156 | val loss 1.12415 acc: 0.66396 iteration: 1776\n",
      "batch 870 loss: 1.48986 acc: 0.58047 | val loss 1.46350 acc: 0.58057 iteration: 1777\n",
      "batch 871 loss: 1.29958 acc: 0.63584 | val loss 1.42614 acc: 0.53086 iteration: 1778\n",
      "batch 872 loss: 1.46573 acc: 0.57529 | val loss 1.54736 acc: 0.56758 iteration: 1779\n",
      "batch 873 loss: 1.45767 acc: 0.61182 | val loss 1.38138 acc: 0.56289 iteration: 1780\n",
      "batch 874 loss: 1.44120 acc: 0.57939 | val loss 0.85790 acc: 0.77969 iteration: 1781\n",
      "batch 875 loss: 1.39185 acc: 0.60654 | val loss 1.01695 acc: 0.71055 iteration: 1782\n",
      "batch 876 loss: 1.47368 acc: 0.58643 | val loss 1.01813 acc: 0.68916 iteration: 1783\n",
      "batch 877 loss: 1.41686 acc: 0.60479 | val loss 1.98947 acc: 0.41836 iteration: 1784\n",
      "batch 878 loss: 1.53848 acc: 0.56650 | val loss 1.89491 acc: 0.51963 iteration: 1785\n",
      "batch 879 loss: 1.33555 acc: 0.62129 | val loss 1.62903 acc: 0.59102 iteration: 1786\n",
      "batch 880 loss: 1.36648 acc: 0.59189 | val loss 1.49997 acc: 0.51787 iteration: 1787\n",
      "batch 881 loss: 1.38250 acc: 0.59355 | val loss 1.40356 acc: 0.58037 iteration: 1788\n",
      "batch 882 loss: 1.38395 acc: 0.59336 | val loss 1.38388 acc: 0.59580 iteration: 1789\n",
      "batch 883 loss: 1.43201 acc: 0.58818 | val loss 1.66858 acc: 0.48809 iteration: 1790\n",
      "batch 884 loss: 1.50228 acc: 0.58047 | val loss 1.26542 acc: 0.60986 iteration: 1791\n",
      "batch 885 loss: 1.45826 acc: 0.56738 | val loss 1.10657 acc: 0.66357 iteration: 1792\n",
      "batch 886 loss: 1.45867 acc: 0.57119 | val loss 1.15024 acc: 0.64541 iteration: 1793\n",
      "batch 887 loss: 1.26469 acc: 0.62139 | val loss 1.21866 acc: 0.63818 iteration: 1794\n",
      "batch 888 loss: 1.41859 acc: 0.59688 | val loss 1.56937 acc: 0.52061 iteration: 1795\n",
      "batch 889 loss: 1.51134 acc: 0.56855 | val loss 1.64371 acc: 0.47588 iteration: 1796\n",
      "batch 890 loss: 1.57328 acc: 0.55107 | val loss 1.58996 acc: 0.50684 iteration: 1797\n",
      "batch 891 loss: 1.45160 acc: 0.58730 | val loss 2.33193 acc: 0.38877 iteration: 1798\n",
      "batch 892 loss: 1.48673 acc: 0.55781 | val loss 1.53007 acc: 0.51924 iteration: 1799\n",
      "batch 893 loss: 1.37019 acc: 0.59844 | val loss 1.25570 acc: 0.63350 iteration: 1800\n",
      "batch 894 loss: 1.48039 acc: 0.56777 | val loss 1.56221 acc: 0.57598 iteration: 1801\n",
      "batch 895 loss: 1.48918 acc: 0.56484 | val loss 2.20427 acc: 0.47490 iteration: 1802\n",
      "batch 896 loss: 1.32925 acc: 0.61748 | val loss 1.23832 acc: 0.64727 iteration: 1803\n",
      "batch 897 loss: 1.34429 acc: 0.60752 | val loss 1.40321 acc: 0.64014 iteration: 1804\n",
      "batch 898 loss: 1.39854 acc: 0.59512 | val loss 1.32968 acc: 0.61025 iteration: 1805\n",
      "batch 899 loss: 1.34245 acc: 0.62305 | val loss 1.06452 acc: 0.64658 iteration: 1806\n",
      "batch 900 loss: 1.35881 acc: 0.58672 | val loss 1.10053 acc: 0.63359 iteration: 1807\n",
      "batch 901 loss: 1.46921 acc: 0.57920 | val loss 1.39040 acc: 0.57285 iteration: 1808\n",
      "batch 902 loss: 1.41917 acc: 0.59414 | val loss 1.87871 acc: 0.50400 iteration: 1809\n",
      "batch 903 loss: 1.50986 acc: 0.56670 | val loss 1.24852 acc: 0.65928 iteration: 1810\n",
      "batch 904 loss: 1.39610 acc: 0.59648 | val loss 1.52396 acc: 0.56758 iteration: 1811\n",
      "batch 905 loss: 1.54116 acc: 0.56357 | val loss 1.90536 acc: 0.45332 iteration: 1812\n",
      "epoch 1 loss: 1.58572 acc: 0.54651 | val loss 1.68043 acc: 0.52207 iteration: 1812\n"
     ]
    }
   ],
   "source": [
    "# rename the experiment dir name every time when you tune the hyperparameters\n",
    "experiment_dir = os.path.join(\"experiment_data/\", experiment_name)\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "training_accs = []\n",
    "val_accs = []\n",
    "\n",
    "best_loss = 1000\n",
    "\n",
    "# start training\n",
    "logs = []\n",
    "device = torch.device(torch.cuda.current_device())\n",
    "iteration = 0\n",
    "step = 0\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch: %d\\n__________________________________________\" % (epoch), flush = True)\n",
    "    mean_loss = 0.0\n",
    "    mean_acc = 0.0\n",
    "    v_mean_loss = 0.0\n",
    "    v_mean_acc = 0.0\n",
    "    total = 0\n",
    "    for i, d in enumerate(tqdm(train_set)):\n",
    "        # validate display\n",
    "        x = gd = d[0]\n",
    "        model.train()\n",
    "        j = i % len(validate_data)\n",
    "        v_x = v_gd = validate_data[j]\n",
    "        \n",
    "        x = x.to(device = device,non_blocking = True)\n",
    "        gd = gd.to(device = device,non_blocking = True)\n",
    "        v_x = v_x.to(device = device,non_blocking = True)\n",
    "        v_gd = v_gd.to(device = device,non_blocking = True)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        recon, r_dis, iteration = model(x, gd)\n",
    "        \n",
    "        acc, loss = loss_function(recon, gd.view(-1), r_dis, vae_beta)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        mean_loss += loss.item()\n",
    "        mean_acc += acc.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            v_recon, v_r_dis, _ = model(v_x, v_gd)\n",
    "            v_acc, v_loss = loss_function(v_recon, v_gd.view(-1), v_r_dis, vae_beta)\n",
    "            v_mean_loss += v_loss.item()\n",
    "            v_mean_acc += v_acc.item()\n",
    "        step += 1\n",
    "        total += 1\n",
    "        if decay > 0:\n",
    "            scheduler.step()\n",
    "        # print(\"batch %d loss: %.5f acc: %.5f | val loss %.5f acc: %.5f iteration: %d\"  \n",
    "        #       % (i,loss.item(), acc.item(), v_loss.item(),v_acc.item(),iteration),flush = True)\n",
    "    mean_loss /= total\n",
    "    mean_acc /= total\n",
    "    v_mean_loss /= total\n",
    "    v_mean_acc /= total\n",
    "\n",
    "    record_stats(mean_loss, mean_acc, v_mean_loss, v_mean_acc)\n",
    "\n",
    "    print(\"epoch %d loss: %.5f acc: %.5f | val loss %.5f acc: %.5f iteration: %d\"  \n",
    "              % (epoch, mean_loss, mean_acc, v_mean_loss, v_mean_acc, iteration),flush = True)\n",
    "    logs.append([mean_loss,mean_acc,v_mean_loss,v_mean_acc,iteration])\n",
    "    if (epoch + 1) % save_period == 0:\n",
    "        filename = \"sketchvae-\" + 'loss_' + str(mean_loss) + \"_\" + str(epoch+1) + \"_\" + str(iteration) + \".pt\"\n",
    "        torch.save(model.cpu().state_dict(),save_path + filename)\n",
    "        model.cuda()\n",
    "    np.save(\"sketchvae-log.npy\", logs)\n",
    "        \n",
    "    if v_mean_loss < best_loss:\n",
    "        save_model(model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(experiment_dir, exist_ok=True)\n",
    "# record_stats(mean_loss, mean_acc, v_mean_loss, v_mean_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}