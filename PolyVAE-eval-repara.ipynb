{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct the latent vectors through vae\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import sklearn.utils\n",
    "from torch.distributions import kl_divergence,Normal\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.nn import functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import PolyVAE_repara\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "class MinExponentialLR(ExponentialLR):\n",
    "    def __init__(self, optimizer, gamma, minimum, last_epoch=-1):\n",
    "        self.min = minimum\n",
    "        super(MinExponentialLR, self).__init__(optimizer, gamma, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [\n",
    "            max(base_lr * self.gamma**self.last_epoch, self.min)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "###############################\n",
    "# initial parameters\n",
    "s_dir = \"\"\n",
    "batch_size = 64\n",
    "data_path = [s_dir + \"data/poly_train.npy\",\n",
    "             s_dir + \"data/poly_validate.npy\",\n",
    "             s_dir + \"data/poly_train.npy\"]\n",
    "save_path = s_dir + \"model_backup/\"\n",
    "lr = 1e-4\n",
    "decay = 0.9999\n",
    "hidden_dims = 1024\n",
    "z_dims = 1024\n",
    "vae_beta = 0.1\n",
    "input_dims = 130\n",
    "seq_len = 10 * 16\n",
    "beat_num = 10\n",
    "tick_num = 16\n",
    "save_period = 1\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58041, 160])\n",
      "torch.Size([7578, 160])\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "train_set = np.load(data_path[0], allow_pickle = True)\n",
    "validate_set = np.load(data_path[1],allow_pickle = True) \n",
    "\n",
    "train_x = []\n",
    "for i,data in enumerate(train_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    train_x.append(temp)\n",
    "train_x = np.array(train_x)\n",
    "# print(train_x.shape)\n",
    "\n",
    "validate_x = []\n",
    "for i,data in enumerate(validate_set):\n",
    "    temp = []\n",
    "    for d in data[\"layers\"]:\n",
    "        temp += d\n",
    "    validate_x.append(temp)\n",
    "validate_x = np.array(validate_x)\n",
    "# print(train_x.shape)\n",
    "train_x = torch.from_numpy(train_x).long()\n",
    "validate_x = torch.from_numpy(validate_x).long()\n",
    "\n",
    "print(train_x.size())\n",
    "print(validate_x.size())\n",
    "\n",
    "train_set = TensorDataset(train_x)\n",
    "validate_set = TensorDataset(validate_x)\n",
    "\n",
    "train_set = DataLoader(\n",
    "    dataset = train_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "validate_set = DataLoader(\n",
    "    dataset = validate_set,\n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = 8, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "model = PolyVAE_repara(input_dims, hidden_dims, z_dims, seq_len, beat_num, tick_num, 4000)\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "dic = torch.load(\"model_backup/polyvae-loss_0.08171498498346895_100_90600.pt\")\n",
    "\n",
    "for name in list(dic.keys()):\n",
    "    dic[name.replace('module.', '')] = dic.pop(name)\n",
    "model.load_state_dict(dic)\n",
    "\n",
    "\n",
    "if decay > 0:\n",
    "    scheduler = MinExponentialLR(optimizer, gamma = decay, minimum = 1e-5)\n",
    "if torch.cuda.is_available():\n",
    "    print('Using: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using: CPU')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon, target, z_mean, z_std, beta):\n",
    "    CE = F.cross_entropy(recon.view(-1, recon.size(-1)), target, reduction = \"mean\")\n",
    "#     rhy_CE = F.nll_loss(recon_rhythm.view(-1, recon_rhythm.size(-1)), target_rhythm, reduction = \"mean\")\n",
    "    \n",
    "    # normal1 =  std_normal(r_dis.mean.size())\n",
    "    # KLD1 = kl_divergence(r_dis, normal1).mean()\n",
    "    mean_sq=z_mean * z_mean\n",
    "    std_sq=z_std * z_std\n",
    "    KLD1 = 0.5 * torch.mean(mean_sq + std_sq - torch.log(std_sq) - 1)\n",
    "\n",
    "    max_indices = recon.view(-1, recon.size(-1)).max(-1)[-1]\n",
    "#     print(max_indices)\n",
    "    correct = max_indices == target\n",
    "    acc = torch.sum(correct.float()) / target.size(0)\n",
    "    return acc, CE + beta * (KLD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8075195550918579\n",
      "1 0.7847168147563934\n",
      "2 0.8089192907015482\n",
      "3 0.8451904505491257\n",
      "4 0.8512890815734864\n",
      "5 0.8511067926883698\n",
      "6 0.8648856197084699\n",
      "7 0.8688110485672951\n",
      "8 0.8665473196241591\n",
      "9 0.869902354478836\n",
      "10 0.8755681948228315\n",
      "11 0.878059908747673\n",
      "12 0.8714693647164565\n",
      "13 0.8679059829030719\n",
      "14 0.873522146542867\n",
      "15 0.877575695514679\n",
      "16 0.8826746449751013\n",
      "17 0.882850491338306\n",
      "18 0.883254537456914\n",
      "19 0.8835254043340683\n",
      "20 0.8834728570211501\n",
      "21 0.8777121955698187\n",
      "22 0.8804432905238607\n",
      "23 0.8781901200612386\n",
      "24 0.8790117335319519\n",
      "25 0.8804950576562148\n",
      "26 0.8823640191996539\n",
      "27 0.8832833575350898\n",
      "28 0.8851124870366064\n",
      "29 0.8856770972410838\n",
      "30 0.8834079023330442\n",
      "31 0.8826873917132616\n",
      "32 0.8820490186864679\n",
      "33 0.8815918091465446\n",
      "34 0.8767829367092678\n",
      "35 0.8762858195437325\n",
      "36 0.8781487667882765\n",
      "37 0.8794921997346377\n",
      "38 0.880013033365592\n",
      "39 0.8761670053005218\n",
      "40 0.8773366169231694\n",
      "41 0.8781505894093287\n",
      "42 0.878615565078203\n",
      "43 0.8799183368682861\n",
      "44 0.8804123388396369\n",
      "45 0.8815684564735579\n",
      "46 0.881507657943888\n",
      "47 0.8819742960234483\n",
      "48 0.8837771172426185\n",
      "49 0.8853183722496033\n",
      "50 0.8871668331763324\n",
      "51 0.8852633100289565\n",
      "52 0.8822910661967296\n",
      "53 0.8812319294170097\n",
      "54 0.881999304077842\n",
      "55 0.8828787814293589\n",
      "56 0.8834053047916346\n",
      "57 0.8818443705295694\n",
      "58 0.882013057248067\n",
      "59 0.8831608215967814\n",
      "60 0.8842229266635707\n",
      "61 0.8852617961745108\n",
      "62 0.885470934330471\n",
      "63 0.8853286886587739\n",
      "64 0.8848557839026818\n",
      "65 0.8830329811934269\n",
      "66 0.882453955821137\n",
      "67 0.8831543116008534\n",
      "68 0.8824728407721588\n",
      "69 0.8806543111801147\n",
      "70 0.88138755862142\n",
      "71 0.881974298093054\n",
      "72 0.8820125354479437\n",
      "73 0.8829524001559695\n",
      "74 0.8838463679949442\n",
      "75 0.884211824912774\n",
      "76 0.8839285853621247\n",
      "77 0.8837990921277267\n",
      "78 0.8835678040226803\n",
      "79 0.8823401011526585\n",
      "80 0.8826196120109087\n",
      "81 0.8809844357211415\n",
      "82 0.8779061687998024\n",
      "83 0.8757905633676619\n",
      "84 0.8749494615723105\n",
      "85 0.8753747406393982\n",
      "86 0.8754804367306589\n",
      "87 0.8758323104544119\n",
      "88 0.876294780983014\n",
      "89 0.8761067834165361\n",
      "90 0.8760087693130577\n",
      "91 0.8751252669355144\n",
      "92 0.8737798339577132\n",
      "93 0.8720526631842268\n",
      "94 0.8711595516455801\n",
      "95 0.8710907101631165\n",
      "96 0.8721679805480328\n",
      "97 0.8732362201019209\n",
      "98 0.8731978096143164\n",
      "99 0.8735781365633011\n",
      "100 0.8736308901616843\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "model.eval()\n",
    "device = torch.device(torch.cuda.current_device())\n",
    "mean_acc = 0.0\n",
    "ys = []\n",
    "gds = []\n",
    "output = []\n",
    "for i, d in enumerate(validate_set):\n",
    "    # validate display\n",
    "    x = gd = d[0]\n",
    "        \n",
    "    x = x.to(device = device,non_blocking = True)\n",
    "    gd = gd.to(device = device,non_blocking = True)\n",
    "            \n",
    "    recon, r_dis, iteration, z_mu, z_var = model(x, gd)\n",
    "        \n",
    "    acc, _ = loss_function(recon, gd.view(-1), z_mu, z_var, vae_beta)\n",
    "    mean_acc += acc.item()\n",
    "\n",
    "#     z = r_dis.rsample()\n",
    "    pred = recon.argmax(-1)\n",
    "    output.append({\"gd\":x.cpu().detach().numpy(), \"pred\":pred.cpu().detach().numpy(), \"acc\": acc.item()})\n",
    "#     print(pred.size())\n",
    "#     print(v_gd.size())\n",
    "#     output.append([pred.cpu().detach().numpy(), v_gd.cpu().detach().numpy()])\n",
    "#     print(v_loss.item())\n",
    "    print(i, mean_acc / (i + 1))\n",
    "    if i == 100:\n",
    "        break\n",
    "    # np.save(\"dismeasurevae_gen_exp.npy\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"reconstruction_2.npy\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
